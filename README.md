# Qwen3-0.6B Terminal Command Generator

A fine-tuned Qwen3-0.6B model that generates terminal commands from natural language instructions. Supports **Linux**, **Windows**, and **macOS**.

## ğŸ¯ Project Overview

This project fine-tunes a [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B) language model using **QLoRA** (Quantized Low-Rank Adaptation) to generate accurate terminal commands from natural language descriptions.

### Key Features

- **Multi-OS Support**: Generates commands for Linux, Windows, and macOS
- **JSON Output**: Can return commands for all operating systems in JSON format
- **High Accuracy**: Achieves ~93-97% exact match accuracy
- **Efficient Training**: Uses QLoRA for memory-efficient fine-tuning

## ğŸ“Š Performance

| Metric | Score |
|--------|-------|
| Exact Match Accuracy | ~93-97% |
| Fuzzy Match Accuracy | ~94-98% |

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Eng-Elias/qwen3-600M-terminal-instruct.git
cd qwen3-600M-terminal-instruct

# Install dependencies
pip install -r requirements.txt

# For CUDA support the command in cuda_requirements.txt
```

### Using the Model

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model from HuggingFace
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.6B")
model = PeftModel.from_pretrained(base_model, "Eng-Elias/qwen3-0.6b-terminal-instruct")
tokenizer = AutoTokenizer.from_pretrained("Eng-Elias/qwen3-0.6b-terminal-instruct")

# Generate command
def generate_command(instruction, os_tag="[LINUX]"):
    prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{os_tag}\n\n### Response:\n"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("### Response:")[-1].strip()

# Examples
print(generate_command("List all files including hidden ones", "[LINUX]"))
# Output: ls -a

print(generate_command("Create a new folder named projects", "[WINDOWS]"))
# Output: mkdir projects

print(generate_command("Show disk usage", "[MAC]"))
# Output: df -h
```

### JSON Output (All OS)

```python
generate_command("Delete file named temp.txt", "Return the command for all operating systems as JSON")
# Output: {"description": "Delete file named temp.txt", "linux": "rm temp.txt", "windows": "del temp.txt", "mac": "rm temp.txt"}
```

## ğŸ“ Project Structure

```
qwen3-600M-terminal-instruct/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_train_evaluate_publish.ipynb  # Training and evaluation
â”‚   â”œâ”€â”€ 02_evaluate_all_sources.ipynb    # Evaluate from all sources
â”‚   â”œâ”€â”€ 03_load_and_test_all.ipynb       # Interactive testing
â”‚   â””â”€â”€ 04_push_model_cards.ipynb        # Push model cards to HF
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ generated/                        # Training data
â”œâ”€â”€ dataset_preprocessing_scripts/        # Data preparation scripts
â”œâ”€â”€ model_cards/                          # HuggingFace model cards
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ lora_adapters/                   # Trained LoRA adapters
â”‚   â”œâ”€â”€ merged_model/                    # Merged model
â”‚   â””â”€â”€ eval_results/                    # Evaluation results
â”œâ”€â”€ requirements.txt                      # Python dependencies
â”œâ”€â”€ cuda_requirements.txt                 # CUDA dependencies
â””â”€â”€ README.md                            # This file
```

## ğŸ““ Notebooks

| Notebook | Description |
|----------|-------------|
| `01_train_evaluate_publish.ipynb` | Main training pipeline: loads data, trains model, evaluates, and publishes to HuggingFace |
| `02_evaluate_all_sources.ipynb` | Evaluates model from 4 sources: local adapters, local merged, HF adapters, HF merged |
| `03_load_and_test_all.ipynb` | Interactive testing playground for all model sources |
| `04_push_model_cards.ipynb` | Pushes README/model cards to HuggingFace repositories |

## ğŸ”§ Training Details

| Parameter | Value |
|-----------|-------|
| Base Model | Qwen/Qwen3-0.6B |
| Method | QLoRA (4-bit NF4 quantization) |
| Dataset | Custom terminal command dataset (10000+ examples) |
| LoRA Rank (r) | 16 |
| LoRA Alpha | 32 |
| Target Modules | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj |
| Learning Rate | 2e-4 |
| Training Steps | ~1800 |
| Batch Size | 4 (Ã—4 gradient accumulation) |

## ğŸŒ HuggingFace Models

| Repository | Description |
|------------|-------------|
| [Eng-Elias/qwen3-0.6b-terminal-instruct](https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct) | Main model repository |
| [Eng-Elias/qwen3-0.6b-terminal-instruct-lora](https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct-lora) | LoRA adapters only |

## ğŸ“‹ Supported Command Categories

| Category | Examples |
|----------|----------|
| **File Operations** | list, copy, move, delete, find, rename |
| **Directory Operations** | create, remove, navigate, list contents |
| **System Info** | disk usage, memory, CPU, processes |
| **Text Processing** | grep, sed, awk, sort, uniq |
| **Network** | ping, curl, wget, netstat, ssh |
| **Compression** | tar, zip, gzip, unzip |
| **Permissions** | chmod, chown, icacls |
| **Package Management** | apt, yum, brew, choco |

## âš ï¸ Limitations

- Commands are based on common usage patterns; complex or obscure commands may not be accurate
- The model may occasionally generate slightly different but functionally equivalent commands
- JSON output format is consistent but may vary in structure for edge cases
- Trained primarily on English instructions

## ğŸ“„ License

This project is licensed under the **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)**.

You are free to:
- **Share** â€” copy and redistribute the material in any medium or format
- **Adapt** â€” remix, transform, and build upon the material

Under the following terms:
- **Attribution** â€” You must give appropriate credit
- **NonCommercial** â€” You may not use the material for commercial purposes
- **ShareAlike** â€” If you remix, transform, or build upon the material, you must distribute your contributions under the same license

Full license: https://creativecommons.org/licenses/by-nc-sa-4.0/legalcode

## ğŸ“š Citation

```bibtex
@misc{qwen3-terminal-instruct,
  author = {Eng-Elias},
  title = {Qwen3-0.6B Terminal Command Generator},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/Eng-Elias/qwen3-600M-terminal-instruct}
}
```

## ğŸ™ Acknowledgments

- [Qwen Team](https://github.com/QwenLM/Qwen) for the base model
- [Hugging Face](https://huggingface.co/) for the transformers and PEFT libraries
- [Ready Tensor](https://www.readytensor.ai/) for the LLM Engineering and Deployment program

## ğŸ‘¤ Author

**Eng. Elias Owis**

- GitHub: [@Eng-Elias](https://github.com/Eng-Elias)
- HuggingFace: [Eng-Elias](https://huggingface.co/Eng-Elias)
