{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push README and Model Cards to HuggingFace\n",
        "\n",
        "This notebook pushes README.md files (model cards) to both HuggingFace repositories:\n",
        "\n",
        "1. **LoRA Adapters Repo:** `Eng-Elias/qwen3-0.6b-terminal-instruct-lora`\n",
        "2. **Merged Model Repo:** `Eng-Elias/qwen3-0.6b-terminal-instruct`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries imported\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from getpass import getpass\n",
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "print(\"âœ… Libraries imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "CONFIGURATION\n",
            "==================================================\n",
            "\n",
            "Repositories:\n",
            "  Adapters: Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
            "  Merged:   Eng-Elias/qwen3-0.6b-terminal-instruct\n",
            "\n",
            "Model Cards:\n",
            "  Adapters: ../model_cards/README_adapters.md\n",
            "  Merged:   ../model_cards/README_merged.md\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "HF_USERNAME = \"Eng-Elias\"  # <-- Change this if needed\n",
        "\n",
        "CONFIG = {\n",
        "    # HuggingFace repos\n",
        "    \"hf_adapter_repo\": f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct-lora\",\n",
        "    \"hf_merged_repo\": f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct\",\n",
        "    \n",
        "    # Model card files\n",
        "    \"model_cards_dir\": \"../model_cards\",\n",
        "    \"readme_adapters\": \"../model_cards/README_adapters.md\",\n",
        "    \"readme_merged\": \"../model_cards/README_merged.md\",\n",
        "}\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nRepositories:\")\n",
        "print(f\"  Adapters: {CONFIG['hf_adapter_repo']}\")\n",
        "print(f\"  Merged:   {CONFIG['hf_merged_repo']}\")\n",
        "print(f\"\\nModel Cards:\")\n",
        "print(f\"  Adapters: {CONFIG['readme_adapters']}\")\n",
        "print(f\"  Merged:   {CONFIG['readme_merged']}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get HuggingFace token from user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Enter your HuggingFace token:\n",
            "   (Get it from: https://huggingface.co/settings/tokens)\n",
            "\n",
            "âœ… Token received\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Logged in to HuggingFace\n",
            "âœ… HuggingFace API initialized\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ” Enter your HuggingFace token:\")\n",
        "print(\"   (Get it from: https://huggingface.co/settings/tokens)\")\n",
        "print()\n",
        "\n",
        "HF_TOKEN = getpass(\"HF Token: \")\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"âŒ No token provided!\")\n",
        "else:\n",
        "    print(\"âœ… Token received\")\n",
        "    \n",
        "    # Login to HuggingFace\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"âœ… Logged in to HuggingFace\")\n",
        "    \n",
        "    # Initialize API\n",
        "    api = HfApi()\n",
        "    print(\"âœ… HuggingFace API initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Preview Model Cards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“„ PREVIEW: LoRA Adapters README\n",
            "============================================================\n",
            "---\n",
            "license: cc-by-nc-sa-4.0\n",
            "base_model: Qwen/Qwen3-0.6B\n",
            "tags:\n",
            "  - terminal\n",
            "  - command-line\n",
            "  - cli\n",
            "  - bash\n",
            "  - powershell\n",
            "  - fine-tuned\n",
            "  - lora\n",
            "  - peft\n",
            "  - qwen\n",
            "language:\n",
            "  - en\n",
            "pipeline_tag: text-generation\n",
            "library_name: peft\n",
            "---\n",
            "\n",
            "# Qwen3-0.6B Terminal Command Generator - LoRA Adapters\n",
            "\n",
            "This repository contains **LoRA adapters** for generating terminal commands from natural language instructions.\n",
            "\n",
            "## Model Description\n",
            "\n",
            "- **Base Model:** [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\n",
            "- **Fine-tuning Method:** QLoRA (4-bit quantization + LoRA)\n",
            "- **Task:** Natural language to terminal command generation\n",
            "- **Supported OS:** Linux, Windows, macOS\n",
            "\n",
            "## Performance\n",
            "\n",
            "| Metric | Score |\n",
            "|--------|-------|\n",
            "| Exact Match Accuracy | ~93-97% |\n",
            "| Fuzzy Match Accuracy | ~94-98% |\n",
            "\n",
            "## Quick Start\n",
            "\n",
            "```python\n",
            "from peft import PeftModel\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "# Load base model and apply LoRA adapters\n",
            "base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
            "model = PeftModel.from_pretrained(base_model, \"Eng-Elias/qwen3-0.6b-terminal-instruct-lora\")\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"Eng-Elias/qwen3-0.6b-terminal-instruct-lora\")\n",
            "\n",
            "# Generate command\n",
            "def generate_command(instruction, os_tag=\"[LINUX]\"):\n",
            "\n",
            "... [162 total lines]\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“„ PREVIEW: LoRA Adapters README\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with open(CONFIG[\"readme_adapters\"], \"r\", encoding=\"utf-8\") as f:\n",
        "    adapters_readme = f.read()\n",
        "    \n",
        "# Show first 50 lines\n",
        "lines = adapters_readme.split(\"\\n\")[:50]\n",
        "print(\"\\n\".join(lines))\n",
        "print(f\"\\n... [{len(adapters_readme.split(chr(10)))} total lines]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“„ PREVIEW: Merged Model README\n",
            "============================================================\n",
            "---\n",
            "license: cc-by-nc-sa-4.0\n",
            "base_model: Qwen/Qwen3-0.6B\n",
            "tags:\n",
            "  - terminal\n",
            "  - command-line\n",
            "  - cli\n",
            "  - bash\n",
            "  - powershell\n",
            "  - fine-tuned\n",
            "  - lora\n",
            "  - peft\n",
            "  - qwen\n",
            "language:\n",
            "  - en\n",
            "pipeline_tag: text-generation\n",
            "library_name: peft\n",
            "---\n",
            "\n",
            "# Qwen3-0.6B Terminal Command Generator\n",
            "\n",
            "A fine-tuned Qwen3-0.6B model for generating terminal commands from natural language instructions. Supports Linux, Windows, and macOS.\n",
            "\n",
            "## Model Description\n",
            "\n",
            "- **Base Model:** [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\n",
            "- **Fine-tuning Method:** QLoRA (4-bit quantization + LoRA)\n",
            "- **Task:** Natural language to terminal command generation\n",
            "- **Supported OS:** Linux, Windows, macOS\n",
            "\n",
            "## Performance\n",
            "\n",
            "| Metric | Score |\n",
            "|--------|-------|\n",
            "| Exact Match Accuracy | ~93-97% |\n",
            "| Fuzzy Match Accuracy | ~94-98% |\n",
            "\n",
            "## Quick Start\n",
            "\n",
            "```python\n",
            "from peft import PeftModel\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "# Load model (base + adapters)\n",
            "base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
            "model = PeftModel.from_pretrained(base_model, \"Eng-Elias/qwen3-0.6b-terminal-instruct\")\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"Eng-Elias/qwen3-0.6b-terminal-instruct\")\n",
            "\n",
            "# Generate command\n",
            "def generate_command(instruction, os_tag=\"[LINUX]\"):\n",
            "\n",
            "... [199 total lines]\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“„ PREVIEW: Merged Model README\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with open(CONFIG[\"readme_merged\"], \"r\", encoding=\"utf-8\") as f:\n",
        "    merged_readme = f.read()\n",
        "    \n",
        "# Show first 50 lines\n",
        "lines = merged_readme.split(\"\\n\")[:50]\n",
        "print(\"\\n\".join(lines))\n",
        "print(f\"\\n... [{len(merged_readme.split(chr(10)))} total lines]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Push README to LoRA Adapters Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“¤ PUSHING README TO LORA ADAPTERS REPO\n",
            "============================================================\n",
            "Repository: Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
            "\n",
            "âœ… README.md pushed successfully!\n",
            "ðŸ”— View at: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“¤ PUSHING README TO LORA ADAPTERS REPO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Repository: {CONFIG['hf_adapter_repo']}\")\n",
        "\n",
        "try:\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=CONFIG[\"readme_adapters\"],\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=CONFIG[\"hf_adapter_repo\"],\n",
        "        repo_type=\"model\",\n",
        "        commit_message=\"Update model card with documentation\"\n",
        "    )\n",
        "    print(\"\\nâœ… README.md pushed successfully!\")\n",
        "    print(f\"ðŸ”— View at: https://huggingface.co/{CONFIG['hf_adapter_repo']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Failed to push: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Push README to Merged Model Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ðŸ“¤ PUSHING README TO MERGED MODEL REPO\n",
            "============================================================\n",
            "Repository: Eng-Elias/qwen3-0.6b-terminal-instruct\n",
            "\n",
            "âœ… README.md pushed successfully!\n",
            "ðŸ”— View at: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“¤ PUSHING README TO MERGED MODEL REPO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Repository: {CONFIG['hf_merged_repo']}\")\n",
        "\n",
        "try:\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=CONFIG[\"readme_merged\"],\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=CONFIG[\"hf_merged_repo\"],\n",
        "        repo_type=\"model\",\n",
        "        commit_message=\"Update model card with documentation\"\n",
        "    )\n",
        "    print(\"\\nâœ… README.md pushed successfully!\")\n",
        "    print(f\"ðŸ”— View at: https://huggingface.co/{CONFIG['hf_merged_repo']}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Failed to push: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "âœ… ALL MODEL CARDS PUSHED!\n",
            "============================================================\n",
            "\n",
            "ðŸ“¦ HuggingFace Repositories:\n",
            "   LoRA Adapters: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
            "   Merged Model:  https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct\n",
            "\n",
            "ðŸ“„ Local Model Card Files:\n",
            "   Adapters: ../model_cards/README_adapters.md\n",
            "   Merged:   ../model_cards/README_merged.md\n",
            "\n",
            "ðŸ“Œ To update model cards:\n",
            "   1. Edit the README files in model_cards/\n",
            "   2. Re-run cells 5 and 6 to push updates\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ALL MODEL CARDS PUSHED!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nðŸ“¦ HuggingFace Repositories:\")\n",
        "print(f\"   LoRA Adapters: https://huggingface.co/{CONFIG['hf_adapter_repo']}\")\n",
        "print(f\"   Merged Model:  https://huggingface.co/{CONFIG['hf_merged_repo']}\")\n",
        "\n",
        "print(\"\\nðŸ“„ Local Model Card Files:\")\n",
        "print(f\"   Adapters: {CONFIG['readme_adapters']}\")\n",
        "print(f\"   Merged:   {CONFIG['readme_merged']}\")\n",
        "\n",
        "print(\"\\nðŸ“Œ To update model cards:\")\n",
        "print(\"   1. Edit the README files in model_cards/\")\n",
        "print(\"   2. Re-run cells 5 and 6 to push updates\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
