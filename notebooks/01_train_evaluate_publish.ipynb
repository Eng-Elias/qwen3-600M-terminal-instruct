{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Evaluate, and Publish Terminal Command Model\n",
    "\n",
    "This notebook:\n",
    "1. Trains Qwen3-0.6B with QLoRA on terminal command dataset\n",
    "2. Evaluates the model while still in memory\n",
    "3. Saves LoRA adapters locally and publishes to HuggingFace\n",
    "4. Saves merged model locally and publishes to HuggingFace\n",
    "\n",
    "**HuggingFace Repos:**\n",
    "- Adapters: `{username}/qwen3-0.6b-terminal-instruct-lora`\n",
    "- Merged: `{username}/qwen3-0.6b-terminal-instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup and CUDA Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CUDA VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… CUDA is available!\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"âŒ CUDA is NOT available!\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from huggingface_hub import HfApi, login\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HUGGINGFACE CONFIGURATION\n",
    "# ============================================\n",
    "HF_USERNAME = \"Eng-Elias\"  # <-- Change this to your HuggingFace username\n",
    "HF_TOKEN = None  # Will prompt if None, or set your token here\n",
    "\n",
    "# Repository names\n",
    "HF_REPO_ADAPTERS = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct-lora\"\n",
    "HF_REPO_MERGED = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct\"\n",
    "\n",
    "# ============================================\n",
    "# MODEL & TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \n",
    "    # Paths\n",
    "    \"train_data\": \"../dataset/generated/processed/train.json\",\n",
    "    \"dev_data\": \"../dataset/generated/processed/dev.json\",\n",
    "    \"test_data\": \"../dataset/generated/processed/test.json\",\n",
    "    \"output_dir\": \"../outputs/terminal_command_model\",\n",
    "    \"adapter_save_path\": \"../outputs/lora_adapters\",\n",
    "    \"merged_model_path\": \"../outputs/merged_model\",\n",
    "    \n",
    "    # Quantization (for 6GB VRAM)\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \n",
    "    # Training Configuration - RTX 2060 Optimized\n",
    "    \"num_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_seq_length\": 256,\n",
    "    \"logging_steps\": 25,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 200,\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # Memory Optimizations\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \n",
    "    # Evaluation\n",
    "    \"max_new_tokens\": 150,\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"HuggingFace Username: {HF_USERNAME}\")\n",
    "print(f\"Adapters Repo: {HF_REPO_ADAPTERS}\")\n",
    "print(f\"Merged Model Repo: {HF_REPO_MERGED}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "if HF_TOKEN is None:\n",
    "    print(\"Please enter your HuggingFace token (get it from https://huggingface.co/settings/tokens):\")\n",
    "    HF_TOKEN = input(\"Token: \")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"âœ… Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_dataset(filepath):\n",
    "    \"\"\"Load dataset from JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = load_json_dataset(CONFIG[\"train_data\"])\n",
    "eval_dataset = load_json_dataset(CONFIG[\"dev_data\"])\n",
    "test_dataset = load_json_dataset(CONFIG[\"test_data\"])\n",
    "\n",
    "print(f\"âœ… Train samples: {len(train_dataset)}\")\n",
    "print(f\"âœ… Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"âœ… Test samples: {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\nğŸ“ Sample data:\")\n",
    "print(json.dumps(train_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded\")\n",
    "\n",
    "# Quantization config\n",
    "print(\"\\nConfiguring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CONFIG[\"load_in_4bit\"],\n",
    "    bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_use_double_quant=CONFIG[\"bnb_4bit_use_double_quant\"],\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Loading model with quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded: {CONFIG['base_model']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   VRAM used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=CONFIG[\"gradient_checkpointing\"]\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRAINABLE PARAMETERS\")\n",
    "print(\"=\" * 50)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nVRAM used after LoRA setup: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format sample into instruction prompt with EOS token.\"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample.get(\"input\", \"\")\n",
    "    output = sample[\"output\"]\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare data for training.\"\"\"\n",
    "    prompts = [format_instruction({\"instruction\": inst, \"input\": inp, \"output\": out}) \n",
    "               for inst, inp, out in zip(examples[\"instruction\"], \n",
    "                                          examples[\"input\"], \n",
    "                                          examples[\"output\"])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_seq_length\"],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"ğŸ“ Sample formatted prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(format_instruction(train_dataset[0]))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing eval\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenized train: {len(tokenized_train)} samples\")\n",
    "print(f\"âœ… Tokenized eval: {len(tokenized_eval)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"adapter_save_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"merged_model_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    optim=CONFIG[\"optim\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized\")\n",
    "print(f\"   Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ STARTING TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âœ… TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Training time: {train_result.metrics['train_runtime'] / 60:.2f} minutes\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Evaluate Model In-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_command(instruction, input_text=\"\", max_new_tokens=150):\n",
    "    \"\"\"Generate command from instruction.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    response = response.split(\"### \")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return pred.strip() == gold.strip()\n",
    "\n",
    "def fuzzy_match(pred, gold):\n",
    "    pred_norm = ' '.join(pred.lower().split())\n",
    "    gold_norm = ' '.join(gold.lower().split())\n",
    "    return pred_norm == gold_norm or gold_norm in pred_norm or pred_norm in gold_norm\n",
    "\n",
    "# Load test data\n",
    "with open(CONFIG[\"test_data\"], 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "single_os_tests = [t for t in test_data if t[\"input\"] in [\"[LINUX]\", \"[WINDOWS]\", \"[MAC]\", \"\"]]\n",
    "json_tests = [t for t in test_data if \"JSON\" in t[\"input\"].upper()]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ” EVALUATING MODEL IN-MEMORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Evaluate single OS commands\n",
    "print(\"\\nğŸ“Š Evaluating Single OS Commands...\")\n",
    "single_results = {\"total\": 0, \"exact_match\": 0, \"fuzzy_match\": 0}\n",
    "\n",
    "sample_size = min(100, len(single_os_tests))  # Sample for speed\n",
    "for sample in tqdm(single_os_tests[:sample_size], desc=\"Evaluating\"):\n",
    "    pred = generate_command(sample[\"instruction\"], sample[\"input\"])\n",
    "    gold = sample[\"output\"]\n",
    "    \n",
    "    single_results[\"total\"] += 1\n",
    "    if exact_match(pred, gold):\n",
    "        single_results[\"exact_match\"] += 1\n",
    "    if fuzzy_match(pred, gold):\n",
    "        single_results[\"fuzzy_match\"] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“Š IN-MEMORY EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total evaluated: {single_results['total']}\")\n",
    "print(f\"Exact match: {single_results['exact_match']} ({100*single_results['exact_match']/single_results['total']:.1f}%)\")\n",
    "print(f\"Fuzzy match: {single_results['fuzzy_match']} ({100*single_results['fuzzy_match']/single_results['total']:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save LoRA Adapters Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ğŸ’¾ SAVING LORA ADAPTERS LOCALLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "adapter_path = CONFIG[\"adapter_save_path\"]\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(adapter_path)\n",
    "print(f\"âœ… LoRA adapters saved to: {adapter_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"âœ… Tokenizer saved to: {adapter_path}\")\n",
    "\n",
    "# Save training config\n",
    "config_save_path = f\"{adapter_path}/training_config.json\"\n",
    "with open(config_save_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"âœ… Config saved to: {config_save_path}\")\n",
    "\n",
    "print(\"\\nğŸ“ Saved adapter files:\")\n",
    "for f in Path(adapter_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Publish LoRA Adapters to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ PUBLISHING LORA ADAPTERS TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_ADAPTERS}\")\n",
    "\n",
    "try:\n",
    "    # Push adapters to HuggingFace\n",
    "    model.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload LoRA adapters for terminal command generation\",\n",
    "        private=False  # Set to True if you want private repo\n",
    "    )\n",
    "    print(\"âœ… LoRA adapters pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    tokenizer.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload tokenizer\"\n",
    "    )\n",
    "    print(\"âœ… Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nğŸ”— View at: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to push adapters: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Create and Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ğŸ”€ CREATING MERGED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ============================================\n",
    "# IMPORTANT: Quantization Mismatch Explanation\n",
    "# ============================================\n",
    "# During QLoRA training, the LoRA adapters learn to compensate for the \n",
    "# specific quantization errors of the 4-bit base model. When merging:\n",
    "# 1. We load the base model in float16 (not quantized)\n",
    "# 2. Apply and merge the LoRA adapters\n",
    "# 3. Save the merged model in float16\n",
    "#\n",
    "# If you later load this merged model with 4-bit quantization, \n",
    "# the NEW quantization pattern differs from training, causing ~17% accuracy drop.\n",
    "#\n",
    "# SOLUTION: Load merged model in float16 (no quantization) for best accuracy,\n",
    "# or accept the accuracy trade-off when using quantization for memory savings.\n",
    "# ============================================\n",
    "\n",
    "print(\"Step 1: Loading base model in float16 (not quantized)...\")\n",
    "\n",
    "# Clear GPU memory from training\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model in full precision for proper merging\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"âœ… Base model loaded in float16\")\n",
    "\n",
    "print(\"\\nStep 2: Loading LoRA adapters...\")\n",
    "model_with_lora = PeftModel.from_pretrained(base_model_for_merge, adapter_path)\n",
    "print(\"âœ… LoRA adapters loaded\")\n",
    "\n",
    "print(\"\\nStep 3: Merging adapters into base model...\")\n",
    "merged_model = model_with_lora.merge_and_unload()\n",
    "print(\"âœ… Adapters merged successfully!\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Verify merged model accuracy (float16, no re-quantization)\n",
    "# ============================================\n",
    "print(\"\\nStep 4: Verifying merged model accuracy (float16)...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def generate_with_model(model_to_use, instruction, input_text=\"\"):\n",
    "    \"\"\"Generate command using specified model.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(model_to_use.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_to_use.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    response = response.split(\"### \")[0].strip()\n",
    "    return response\n",
    "\n",
    "# Quick verification on subset of test data\n",
    "merged_results = {\"total\": 0, \"exact_match\": 0}\n",
    "verification_size = min(50, len(single_os_tests))\n",
    "\n",
    "for sample in tqdm(single_os_tests[:verification_size], desc=\"Verifying merged model\"):\n",
    "    pred = generate_with_model(merged_model, sample[\"instruction\"], sample[\"input\"])\n",
    "    gold = sample[\"output\"]\n",
    "    merged_results[\"total\"] += 1\n",
    "    if pred.strip() == gold.strip():\n",
    "        merged_results[\"exact_match\"] += 1\n",
    "\n",
    "merged_accuracy = 100 * merged_results[\"exact_match\"] / merged_results[\"total\"]\n",
    "print(f\"\\nâœ… Merged model (float16) accuracy: {merged_accuracy:.1f}%\")\n",
    "\n",
    "if merged_accuracy >= 90:\n",
    "    print(\"   âœ… Merged model maintains high accuracy when loaded in float16!\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Unexpected accuracy drop - check merging process\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"ğŸ“Œ NOTE: When loading this merged model with 4-bit quantization,\")\n",
    "print(\"   expect ~15-20% accuracy drop due to quantization mismatch.\")\n",
    "print(\"   For best results, load in float16 without quantization.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 5: Save merged model\n",
    "print(\"\\nStep 5: Saving merged model locally...\")\n",
    "merged_model_path = CONFIG[\"merged_model_path\"]\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(f\"âœ… Merged model saved to: {merged_model_path}\")\n",
    "\n",
    "print(\"\\nğŸ“ Merged model files:\")\n",
    "for f in Path(merged_model_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Publish Merged Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ PUBLISHING MERGED MODEL TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_MERGED}\")\n",
    "\n",
    "try:\n",
    "    # Push merged model to HuggingFace\n",
    "    merged_model.push_to_hub(\n",
    "        HF_REPO_MERGED,\n",
    "        commit_message=\"Upload merged model for terminal command generation\",\n",
    "        private=False\n",
    "    )\n",
    "    print(\"âœ… Merged model pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    tokenizer.push_to_hub(\n",
    "        HF_REPO_MERGED,\n",
    "        commit_message=\"Upload tokenizer\"\n",
    "    )\n",
    "    print(\"âœ… Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nğŸ”— View at: https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "    \n",
    "    # Important usage note\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âš ï¸ IMPORTANT: LOADING INSTRUCTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"For BEST accuracy (~97%), load WITHOUT quantization:\")\n",
    "    print(\"```python\")\n",
    "    print(\"model = AutoModelForCausalLM.from_pretrained(\")\n",
    "    print(f'    \"{HF_REPO_MERGED}\",')\n",
    "    print(\"    torch_dtype=torch.float16,\")\n",
    "    print(\"    device_map=\\\"auto\\\"\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    print(\"\\nWith 4-bit quantization, expect ~80-83% accuracy due to\")\n",
    "    print(\"quantization mismatch from QLoRA training.\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to push merged model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ALL TASKS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“¦ LOCAL SAVES:\")\n",
    "print(f\"   LoRA Adapters: {CONFIG['adapter_save_path']}\")\n",
    "print(f\"   Merged Model: {CONFIG['merged_model_path']}\")\n",
    "\n",
    "print(\"\\nğŸŒ HUGGINGFACE REPOS:\")\n",
    "print(f\"   Adapters: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "print(f\"   Merged:   https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "\n",
    "print(\"\\nğŸ“Š EVALUATION RESULTS:\")\n",
    "print(f\"   In-Memory (LoRA + 4-bit base): {100*single_results['exact_match']/single_results['total']:.1f}%\")\n",
    "print(f\"   Merged Model (float16):        {merged_accuracy:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âš ï¸ QUANTIZATION MISMATCH - IMPORTANT NOTES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "During QLoRA training, LoRA adapters learn to compensate for the specific\n",
    "quantization errors of the 4-bit base model. This creates a mismatch when\n",
    "loading models differently:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Loading Method                          â”‚ Expected Accuracy â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Base (4-bit) + LoRA adapters            â”‚ ~97% (best)       â”‚\n",
    "â”‚ Merged model (float16, no quantization) â”‚ ~97% (best)       â”‚\n",
    "â”‚ Merged model (re-quantized to 4-bit)    â”‚ ~80-83% (degraded)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "1. For production with limited VRAM: Use base model + LoRA adapters\n",
    "2. For production with enough VRAM: Use merged model in float16\n",
    "3. Avoid re-quantizing the merged model (causes accuracy loss)\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Œ NEXT STEPS:\")\n",
    "print(\"   1. Run 02_evaluate_all_sources.ipynb to compare all loading methods\")\n",
    "print(\"   2. Run 03_load_and_test_all.ipynb for interactive testing\")\n",
    "print(\"   3. Update model cards on HuggingFace with loading instructions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
