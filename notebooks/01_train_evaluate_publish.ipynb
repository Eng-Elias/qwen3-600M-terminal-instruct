{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Evaluate, and Publish Terminal Command Model\n",
    "\n",
    "This notebook:\n",
    "1. Trains Qwen3-0.6B with QLoRA on terminal command dataset\n",
    "2. Evaluates the model while still in memory\n",
    "3. Saves LoRA adapters locally and publishes to HuggingFace\n",
    "4. Saves merged model locally and publishes to HuggingFace\n",
    "\n",
    "**HuggingFace Repos:**\n",
    "- Adapters: `{username}/qwen3-0.6b-terminal-instruct-lora`\n",
    "- Merged: `{username}/qwen3-0.6b-terminal-instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup and CUDA Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CUDA VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA is available!\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available!\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from huggingface_hub import HfApi, login\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HUGGINGFACE CONFIGURATION\n",
    "# ============================================\n",
    "HF_USERNAME = \"your-username\"  # <-- Change this to your HuggingFace username\n",
    "HF_TOKEN = None  # Will prompt if None, or set your token here\n",
    "\n",
    "# Repository names\n",
    "HF_REPO_ADAPTERS = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct-lora\"\n",
    "HF_REPO_MERGED = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct\"\n",
    "\n",
    "# ============================================\n",
    "# MODEL & TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \n",
    "    # Paths\n",
    "    \"train_data\": \"../dataset/generated/processed/train.json\",\n",
    "    \"dev_data\": \"../dataset/generated/processed/dev.json\",\n",
    "    \"test_data\": \"../dataset/generated/processed/test.json\",\n",
    "    \"output_dir\": \"../outputs/terminal_command_model\",\n",
    "    \"adapter_save_path\": \"../outputs/lora_adapters\",\n",
    "    \"merged_model_path\": \"../outputs/merged_model\",\n",
    "    \n",
    "    # Quantization (for 6GB VRAM)\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \n",
    "    # Training Configuration - RTX 2060 Optimized\n",
    "    \"num_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_seq_length\": 256,\n",
    "    \"logging_steps\": 25,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 200,\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # Memory Optimizations\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \n",
    "    # Evaluation\n",
    "    \"max_new_tokens\": 150,\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"HuggingFace Username: {HF_USERNAME}\")\n",
    "print(f\"Adapters Repo: {HF_REPO_ADAPTERS}\")\n",
    "print(f\"Merged Model Repo: {HF_REPO_MERGED}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "if HF_TOKEN is None:\n",
    "    print(\"Please enter your HuggingFace token (get it from https://huggingface.co/settings/tokens):\")\n",
    "    HF_TOKEN = input(\"Token: \")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_dataset(filepath):\n",
    "    \"\"\"Load dataset from JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = load_json_dataset(CONFIG[\"train_data\"])\n",
    "eval_dataset = load_json_dataset(CONFIG[\"dev_data\"])\n",
    "test_dataset = load_json_dataset(CONFIG[\"test_data\"])\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\nüìù Sample data:\")\n",
    "print(json.dumps(train_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Quantization config\n",
    "print(\"\\nConfiguring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CONFIG[\"load_in_4bit\"],\n",
    "    bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_use_double_quant=CONFIG[\"bnb_4bit_use_double_quant\"],\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Loading model with quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {CONFIG['base_model']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   VRAM used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=CONFIG[\"gradient_checkpointing\"]\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRAINABLE PARAMETERS\")\n",
    "print(\"=\" * 50)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nVRAM used after LoRA setup: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format sample into instruction prompt with EOS token.\"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample.get(\"input\", \"\")\n",
    "    output = sample[\"output\"]\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare data for training.\"\"\"\n",
    "    prompts = [format_instruction({\"instruction\": inst, \"input\": inp, \"output\": out}) \n",
    "               for inst, inp, out in zip(examples[\"instruction\"], \n",
    "                                          examples[\"input\"], \n",
    "                                          examples[\"output\"])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_seq_length\"],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"üìù Sample formatted prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(format_instruction(train_dataset[0]))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing eval\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenized train: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úÖ Tokenized eval: {len(tokenized_eval)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"adapter_save_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"merged_model_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    optim=CONFIG[\"optim\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print(f\"   Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Training time: {train_result.metrics['train_runtime'] / 60:.2f} minutes\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Evaluate Model In-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_command(instruction, input_text=\"\", max_new_tokens=150):\n",
    "    \"\"\"Generate command from instruction.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    response = response.split(\"### \")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return pred.strip() == gold.strip()\n",
    "\n",
    "def fuzzy_match(pred, gold):\n",
    "    pred_norm = ' '.join(pred.lower().split())\n",
    "    gold_norm = ' '.join(gold.lower().split())\n",
    "    return pred_norm == gold_norm or gold_norm in pred_norm or pred_norm in gold_norm\n",
    "\n",
    "# Load test data\n",
    "with open(CONFIG[\"test_data\"], 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "single_os_tests = [t for t in test_data if t[\"input\"] in [\"[LINUX]\", \"[WINDOWS]\", \"[MAC]\", \"\"]]\n",
    "json_tests = [t for t in test_data if \"JSON\" in t[\"input\"].upper()]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üîç EVALUATING MODEL IN-MEMORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Evaluate single OS commands\n",
    "print(\"\\nüìä Evaluating Single OS Commands...\")\n",
    "single_results = {\"total\": 0, \"exact_match\": 0, \"fuzzy_match\": 0}\n",
    "\n",
    "sample_size = min(100, len(single_os_tests))  # Sample for speed\n",
    "for sample in tqdm(single_os_tests[:sample_size], desc=\"Evaluating\"):\n",
    "    pred = generate_command(sample[\"instruction\"], sample[\"input\"])\n",
    "    gold = sample[\"output\"]\n",
    "    \n",
    "    single_results[\"total\"] += 1\n",
    "    if exact_match(pred, gold):\n",
    "        single_results[\"exact_match\"] += 1\n",
    "    if fuzzy_match(pred, gold):\n",
    "        single_results[\"fuzzy_match\"] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìä IN-MEMORY EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total evaluated: {single_results['total']}\")\n",
    "print(f\"Exact match: {single_results['exact_match']} ({100*single_results['exact_match']/single_results['total']:.1f}%)\")\n",
    "print(f\"Fuzzy match: {single_results['fuzzy_match']} ({100*single_results['fuzzy_match']/single_results['total']:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save LoRA Adapters Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üíæ SAVING LORA ADAPTERS LOCALLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "adapter_path = CONFIG[\"adapter_save_path\"]\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(adapter_path)\n",
    "print(f\"‚úÖ LoRA adapters saved to: {adapter_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"‚úÖ Tokenizer saved to: {adapter_path}\")\n",
    "\n",
    "# Save training config\n",
    "config_save_path = f\"{adapter_path}/training_config.json\"\n",
    "with open(config_save_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"‚úÖ Config saved to: {config_save_path}\")\n",
    "\n",
    "print(\"\\nüìÅ Saved adapter files:\")\n",
    "for f in Path(adapter_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Publish LoRA Adapters to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ PUBLISHING LORA ADAPTERS TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_ADAPTERS}\")\n",
    "\n",
    "try:\n",
    "    # Push adapters to HuggingFace\n",
    "    model.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload LoRA adapters for terminal command generation\",\n",
    "        private=False  # Set to True if you want private repo\n",
    "    )\n",
    "    print(\"‚úÖ LoRA adapters pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    tokenizer.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload tokenizer\"\n",
    "    )\n",
    "    print(\"‚úÖ Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nüîó View at: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to push adapters: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Create and Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üîÄ CREATING MERGED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# For proper merging, we need to:\n",
    "# 1. Load the base model in full precision (not 4-bit)\n",
    "# 2. Apply the LoRA adapters\n",
    "# 3. Merge and save\n",
    "\n",
    "print(\"Step 1: Loading base model in bfloat16/float16 (not quantized)...\")\n",
    "\n",
    "# Clear GPU memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model in full precision\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    torch_dtype=torch.float16,  # Using float16 for RTX 2060\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"‚úÖ Base model loaded in float16\")\n",
    "\n",
    "print(\"\\nStep 2: Loading LoRA adapters...\")\n",
    "model_with_lora = PeftModel.from_pretrained(base_model_for_merge, adapter_path)\n",
    "print(\"‚úÖ LoRA adapters loaded\")\n",
    "\n",
    "print(\"\\nStep 3: Merging adapters into base model...\")\n",
    "merged_model = model_with_lora.merge_and_unload()\n",
    "print(\"‚úÖ Adapters merged successfully!\")\n",
    "\n",
    "print(\"\\nStep 4: Saving merged model locally...\")\n",
    "merged_model_path = CONFIG[\"merged_model_path\"]\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(f\"‚úÖ Merged model saved to: {merged_model_path}\")\n",
    "\n",
    "print(\"\\nüìÅ Merged model files:\")\n",
    "for f in Path(merged_model_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Publish Merged Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ PUBLISHING MERGED MODEL TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_MERGED}\")\n",
    "\n",
    "try:\n",
    "    # Push merged model to HuggingFace\n",
    "    merged_model.push_to_hub(\n",
    "        HF_REPO_MERGED,\n",
    "        commit_message=\"Upload merged model for terminal command generation\",\n",
    "        private=False\n",
    "    )\n",
    "    print(\"‚úÖ Merged model pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    tokenizer.push_to_hub(\n",
    "        HF_REPO_MERGED,\n",
    "        commit_message=\"Upload tokenizer\"\n",
    "    )\n",
    "    print(\"‚úÖ Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nüîó View at: https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to push merged model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ALL TASKS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüì¶ LOCAL SAVES:\")\n",
    "print(f\"   LoRA Adapters: {CONFIG['adapter_save_path']}\")\n",
    "print(f\"   Merged Model: {CONFIG['merged_model_path']}\")\n",
    "\n",
    "print(\"\\nüåê HUGGINGFACE REPOS:\")\n",
    "print(f\"   Adapters: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "print(f\"   Merged:   https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "\n",
    "print(\"\\nüìä IN-MEMORY EVALUATION:\")\n",
    "print(f\"   Exact Match: {100*single_results['exact_match']/single_results['total']:.1f}%\")\n",
    "print(f\"   Fuzzy Match: {100*single_results['fuzzy_match']/single_results['total']:.1f}%\")\n",
    "\n",
    "print(\"\\nüìå NEXT STEPS:\")\n",
    "print(\"   1. Run 02_evaluate_all_sources.ipynb to evaluate all 4 loading methods\")\n",
    "print(\"   2. Run 03_load_and_test_all.ipynb for interactive testing\")\n",
    "print(\"   3. Update model cards on HuggingFace\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
