{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Evaluate, and Publish Terminal Command Model\n",
    "\n",
    "This notebook:\n",
    "1. Trains Qwen3-0.6B with QLoRA on terminal command dataset\n",
    "2. Evaluates the model while still in memory\n",
    "3. Saves LoRA adapters locally and publishes to HuggingFace\n",
    "4. Saves merged model locally and publishes to HuggingFace\n",
    "\n",
    "**HuggingFace Repos:**\n",
    "- Adapters: `{username}/qwen3-0.6b-terminal-instruct-lora`\n",
    "- Merged: `{username}/qwen3-0.6b-terminal-instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup and CUDA Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CUDA VERIFICATION\n",
      "==================================================\n",
      "PyTorch Version: 2.9.0+cu130\n",
      "‚úÖ CUDA is available!\n",
      "   GPU: NVIDIA GeForce RTX 2060\n",
      "   VRAM: 6.00 GB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CUDA VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA is available!\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available!\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from huggingface_hub import HfApi, login\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CONFIGURATION\n",
      "==================================================\n",
      "HuggingFace Username: Eng-Elias\n",
      "Adapters Repo: Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
      "Merged Model Repo: Eng-Elias/qwen3-0.6b-terminal-instruct\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# HUGGINGFACE CONFIGURATION\n",
    "# ============================================\n",
    "HF_USERNAME = \"Eng-Elias\"  # <-- Change this to your HuggingFace username\n",
    "HF_TOKEN = None  # Will prompt if None, or set your token here\n",
    "\n",
    "# Repository names\n",
    "HF_REPO_ADAPTERS = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct-lora\"\n",
    "HF_REPO_MERGED = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct\"\n",
    "\n",
    "# ============================================\n",
    "# MODEL & TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \n",
    "    # Paths\n",
    "    \"train_data\": \"../dataset/generated/processed/train.json\",\n",
    "    \"dev_data\": \"../dataset/generated/processed/dev.json\",\n",
    "    \"test_data\": \"../dataset/generated/processed/test.json\",\n",
    "    \"output_dir\": \"../outputs/terminal_command_model\",\n",
    "    \"adapter_save_path\": \"../outputs/lora_adapters\",\n",
    "    \"merged_model_path\": \"../outputs/merged_model\",\n",
    "    \n",
    "    # Quantization (for 6GB VRAM)\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \n",
    "    # Training Configuration - RTX 2060 Optimized\n",
    "    \"num_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_seq_length\": 256,\n",
    "    \"logging_steps\": 25,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 200,\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # Memory Optimizations\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \n",
    "    # Evaluation\n",
    "    \"max_new_tokens\": 150,\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"HuggingFace Username: {HF_USERNAME}\")\n",
    "print(f\"Adapters Repo: {HF_REPO_ADAPTERS}\")\n",
    "print(f\"Merged Model Repo: {HF_REPO_MERGED}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your HuggingFace token (get it from https://huggingface.co/settings/tokens):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in to HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace\n",
    "if HF_TOKEN is None:\n",
    "    print(\"Please enter your HuggingFace token (get it from https://huggingface.co/settings/tokens):\")\n",
    "    HF_TOKEN = input(\"Token: \")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "‚úÖ Train samples: 9780\n",
      "‚úÖ Eval samples: 1150\n",
      "‚úÖ Test samples: 577\n",
      "\n",
      "üìù Sample data:\n",
      "{\n",
      "  \"instruction\": \"Find all .css files in temp\",\n",
      "  \"input\": \"[LINUX]\",\n",
      "  \"output\": \"find temp -name '*.css'\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def load_json_dataset(filepath):\n",
    "    \"\"\"Load dataset from JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = load_json_dataset(CONFIG[\"train_data\"])\n",
    "eval_dataset = load_json_dataset(CONFIG[\"dev_data\"])\n",
    "test_dataset = load_json_dataset(CONFIG[\"test_data\"])\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\nüìù Sample data:\")\n",
    "print(json.dumps(train_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "‚úÖ Tokenizer loaded\n",
      "\n",
      "Configuring 4-bit quantization...\n",
      "Loading model with quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: Qwen/Qwen3-0.6B\n",
      "   VRAM used: 0.50 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Quantization config\n",
    "print(\"\\nConfiguring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CONFIG[\"load_in_4bit\"],\n",
    "    bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_use_double_quant=CONFIG[\"bnb_4bit_use_double_quant\"],\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Loading model with quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {CONFIG['base_model']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   VRAM used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for LoRA training...\n",
      "\n",
      "==================================================\n",
      "TRAINABLE PARAMETERS\n",
      "==================================================\n",
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7638\n",
      "\n",
      "VRAM used after LoRA setup: 0.81 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=CONFIG[\"gradient_checkpointing\"]\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRAINABLE PARAMETERS\")\n",
    "print(\"=\" * 50)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nVRAM used after LoRA setup: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sample formatted prompt:\n",
      "--------------------------------------------------\n",
      "### Instruction:\n",
      "Find all .css files in temp\n",
      "\n",
      "### Input:\n",
      "[LINUX]\n",
      "\n",
      "### Response:\n",
      "find temp -name '*.css'<|im_end|>\n",
      "--------------------------------------------------\n",
      "\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e841c6a7ea4e05950cc74e0ba9d919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/9780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617704f3206345899d04bb3699cdeca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval:   0%|          | 0/1150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized train: 9780 samples\n",
      "‚úÖ Tokenized eval: 1150 samples\n"
     ]
    }
   ],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format sample into instruction prompt with EOS token.\"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample.get(\"input\", \"\")\n",
    "    output = sample[\"output\"]\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare data for training.\"\"\"\n",
    "    prompts = [format_instruction({\"instruction\": inst, \"input\": inp, \"output\": out}) \n",
    "               for inst, inp, out in zip(examples[\"instruction\"], \n",
    "                                          examples[\"input\"], \n",
    "                                          examples[\"output\"])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_seq_length\"],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"üìù Sample formatted prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(format_instruction(train_dataset[0]))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing eval\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenized train: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úÖ Tokenized eval: {len(tokenized_eval)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized\n",
      "   Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"adapter_save_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"merged_model_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    optim=CONFIG[\"optim\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print(f\"   Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ STARTING TRAINING\n",
      "==================================================\n",
      "Start time: 2025-12-28 20:55:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1836/1836 4:10:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.242588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.131607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.108854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.092977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.081072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.070620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.062895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.059255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.054621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.050464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.047432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.045880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.044305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.043243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.042404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.041836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.041572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.041490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úÖ TRAINING COMPLETED!\n",
      "==================================================\n",
      "End time: 2025-12-29 01:05:33\n",
      "Training time: 250.24 minutes\n",
      "Training loss: 0.3047\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Training time: {train_result.metrics['train_runtime'] / 60:.2f} minutes\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Evaluate Model In-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üîç EVALUATING MODEL IN-MEMORY\n",
      "==================================================\n",
      "\n",
      "üìä Evaluating Single OS Commands...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:46<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä IN-MEMORY EVALUATION RESULTS\n",
      "==================================================\n",
      "Total evaluated: 100\n",
      "Exact match: 96 (96.0%)\n",
      "Fuzzy match: 97 (97.0%)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_command(instruction, input_text=\"\", max_new_tokens=150):\n",
    "    \"\"\"Generate command from instruction.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    response = response.split(\"### \")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return pred.strip() == gold.strip()\n",
    "\n",
    "def fuzzy_match(pred, gold):\n",
    "    pred_norm = ' '.join(pred.lower().split())\n",
    "    gold_norm = ' '.join(gold.lower().split())\n",
    "    return pred_norm == gold_norm or gold_norm in pred_norm or pred_norm in gold_norm\n",
    "\n",
    "# Load test data\n",
    "with open(CONFIG[\"test_data\"], 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "single_os_tests = [t for t in test_data if t[\"input\"] in [\"[LINUX]\", \"[WINDOWS]\", \"[MAC]\", \"\"]]\n",
    "json_tests = [t for t in test_data if \"JSON\" in t[\"input\"].upper()]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üîç EVALUATING MODEL IN-MEMORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Evaluate single OS commands\n",
    "print(\"\\nüìä Evaluating Single OS Commands...\")\n",
    "single_results = {\"total\": 0, \"exact_match\": 0, \"fuzzy_match\": 0}\n",
    "\n",
    "sample_size = min(100, len(single_os_tests))  # Sample for speed\n",
    "for sample in tqdm(single_os_tests[:sample_size], desc=\"Evaluating\"):\n",
    "    pred = generate_command(sample[\"instruction\"], sample[\"input\"])\n",
    "    gold = sample[\"output\"]\n",
    "    \n",
    "    single_results[\"total\"] += 1\n",
    "    if exact_match(pred, gold):\n",
    "        single_results[\"exact_match\"] += 1\n",
    "    if fuzzy_match(pred, gold):\n",
    "        single_results[\"fuzzy_match\"] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìä IN-MEMORY EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total evaluated: {single_results['total']}\")\n",
    "print(f\"Exact match: {single_results['exact_match']} ({100*single_results['exact_match']/single_results['total']:.1f}%)\")\n",
    "print(f\"Fuzzy match: {single_results['fuzzy_match']} ({100*single_results['fuzzy_match']/single_results['total']:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save LoRA Adapters Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üíæ SAVING LORA ADAPTERS LOCALLY\n",
      "==================================================\n",
      "‚úÖ LoRA adapters saved to: ../outputs/lora_adapters\n",
      "‚úÖ Tokenizer saved to: ../outputs/lora_adapters\n",
      "‚úÖ Config saved to: ../outputs/lora_adapters/training_config.json\n",
      "\n",
      "üìÅ Saved adapter files:\n",
      "   adapter_config.json: 0.00 MB\n",
      "   adapter_model.safetensors: 17.53 MB\n",
      "   added_tokens.json: 0.00 MB\n",
      "   chat_template.jinja: 0.00 MB\n",
      "   merges.txt: 1.59 MB\n",
      "   README.md: 0.00 MB\n",
      "   special_tokens_map.json: 0.00 MB\n",
      "   tokenizer.json: 10.89 MB\n",
      "   tokenizer_config.json: 0.01 MB\n",
      "   training_config.json: 0.00 MB\n",
      "   vocab.json: 2.65 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üíæ SAVING LORA ADAPTERS LOCALLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "adapter_path = CONFIG[\"adapter_save_path\"]\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(adapter_path)\n",
    "print(f\"‚úÖ LoRA adapters saved to: {adapter_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"‚úÖ Tokenizer saved to: {adapter_path}\")\n",
    "\n",
    "# Save training config\n",
    "config_save_path = f\"{adapter_path}/training_config.json\"\n",
    "with open(config_save_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"‚úÖ Config saved to: {config_save_path}\")\n",
    "\n",
    "print(\"\\nüìÅ Saved adapter files:\")\n",
    "for f in Path(adapter_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Publish LoRA Adapters to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ PUBLISHING LORA ADAPTERS TO HUGGINGFACE\n",
      "==================================================\n",
      "Repository: Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9290baac00348b9bc136b4999aeebb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc8e03a40b648c1b0376570f9650656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters pushed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db3407eb78c490fa08494d3a4f36edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16066b5690994c8d8ce387906570da5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe60cec6931463a9a563119e2a2714b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer pushed!\n",
      "\n",
      "üîó View at: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ PUBLISHING LORA ADAPTERS TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_ADAPTERS}\")\n",
    "\n",
    "try:\n",
    "    # Push adapters to HuggingFace\n",
    "    model.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload LoRA adapters for terminal command generation\",\n",
    "        private=False  # Set to True if you want private repo\n",
    "    )\n",
    "    print(\"‚úÖ LoRA adapters pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    tokenizer.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload tokenizer\"\n",
    "    )\n",
    "    print(\"‚úÖ Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nüîó View at: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to push adapters: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Create and Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üîÄ CREATING MERGED MODEL\n",
      "==================================================\n",
      "Step 1: Loading base model in float16 (not quantized)...\n",
      "‚úÖ Base model loaded in float16\n",
      "\n",
      "Step 2: Loading LoRA adapters...\n",
      "‚úÖ LoRA adapters loaded\n",
      "\n",
      "Step 3: Merging adapters into base model...\n",
      "‚úÖ Adapters merged successfully!\n",
      "\n",
      "Step 4: Verifying merged model accuracy (float16)...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying merged model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:39<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Merged model (float16) accuracy: 62.0%\n",
      "   ‚ö†Ô∏è Unexpected accuracy drop - check merging process\n",
      "\n",
      "--------------------------------------------------\n",
      "üìå NOTE: When loading this merged model with 4-bit quantization,\n",
      "   expect ~15-20% accuracy drop due to quantization mismatch.\n",
      "   For best results, load in float16 without quantization.\n",
      "--------------------------------------------------\n",
      "\n",
      "Step 5: Saving merged model locally...\n",
      "‚úÖ Merged model saved to: ../outputs/merged_model\n",
      "\n",
      "üìÅ Merged model files:\n",
      "   added_tokens.json: 0.00 MB\n",
      "   chat_template.jinja: 0.00 MB\n",
      "   config.json: 0.00 MB\n",
      "   generation_config.json: 0.00 MB\n",
      "   merges.txt: 1.59 MB\n",
      "   model.safetensors: 1136.91 MB\n",
      "   special_tokens_map.json: 0.00 MB\n",
      "   tokenizer.json: 10.89 MB\n",
      "   tokenizer_config.json: 0.01 MB\n",
      "   vocab.json: 2.65 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üîÄ CREATING MERGED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ============================================\n",
    "# IMPORTANT: Quantization Mismatch Explanation\n",
    "# ============================================\n",
    "# During QLoRA training, the LoRA adapters learn to compensate for the \n",
    "# specific quantization errors of the 4-bit base model. When merging:\n",
    "# 1. We load the base model in float16 (not quantized)\n",
    "# 2. Apply and merge the LoRA adapters\n",
    "# 3. Save the merged model in float16\n",
    "#\n",
    "# If you later load this merged model with 4-bit quantization, \n",
    "# the NEW quantization pattern differs from training, causing ~17% accuracy drop.\n",
    "#\n",
    "# SOLUTION: Load merged model in float16 (no quantization) for best accuracy,\n",
    "# or accept the accuracy trade-off when using quantization for memory savings.\n",
    "# ============================================\n",
    "\n",
    "print(\"Step 1: Loading base model in float16 (not quantized)...\")\n",
    "\n",
    "# Clear GPU memory from training\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model in full precision for proper merging\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"‚úÖ Base model loaded in float16\")\n",
    "\n",
    "print(\"\\nStep 2: Loading LoRA adapters...\")\n",
    "model_with_lora = PeftModel.from_pretrained(base_model_for_merge, adapter_path)\n",
    "print(\"‚úÖ LoRA adapters loaded\")\n",
    "\n",
    "print(\"\\nStep 3: Merging adapters into base model...\")\n",
    "merged_model = model_with_lora.merge_and_unload()\n",
    "print(\"‚úÖ Adapters merged successfully!\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Verify merged model accuracy (float16, no re-quantization)\n",
    "# ============================================\n",
    "print(\"\\nStep 4: Verifying merged model accuracy (float16)...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def generate_with_model(model_to_use, instruction, input_text=\"\"):\n",
    "    \"\"\"Generate command using specified model.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(model_to_use.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_to_use.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    response = response.split(\"### \")[0].strip()\n",
    "    return response\n",
    "\n",
    "# Quick verification on subset of test data\n",
    "merged_results = {\"total\": 0, \"exact_match\": 0}\n",
    "verification_size = min(50, len(single_os_tests))\n",
    "\n",
    "for sample in tqdm(single_os_tests[:verification_size], desc=\"Verifying merged model\"):\n",
    "    pred = generate_with_model(merged_model, sample[\"instruction\"], sample[\"input\"])\n",
    "    gold = sample[\"output\"]\n",
    "    merged_results[\"total\"] += 1\n",
    "    if pred.strip() == gold.strip():\n",
    "        merged_results[\"exact_match\"] += 1\n",
    "\n",
    "merged_accuracy = 100 * merged_results[\"exact_match\"] / merged_results[\"total\"]\n",
    "print(f\"\\n‚úÖ Merged model (float16) accuracy: {merged_accuracy:.1f}%\")\n",
    "\n",
    "if merged_accuracy >= 90:\n",
    "    print(\"   ‚úÖ Merged model maintains high accuracy when loaded in float16!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Unexpected accuracy drop - check merging process\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"üìå NOTE: When loading this merged model with 4-bit quantization,\")\n",
    "print(\"   expect ~15-20% accuracy drop due to quantization mismatch.\")\n",
    "print(\"   For best results, load in float16 without quantization.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 5: Save merged model\n",
    "print(\"\\nStep 5: Saving merged model locally...\")\n",
    "merged_model_path = CONFIG[\"merged_model_path\"]\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(f\"‚úÖ Merged model saved to: {merged_model_path}\")\n",
    "\n",
    "print(\"\\nüìÅ Merged model files:\")\n",
    "for f in Path(merged_model_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Publish Merged Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ PUBLISHING MERGED MODEL TO HUGGINGFACE\n",
      "==================================================\n",
      "Repository: Eng-Elias/qwen3-0.6b-terminal-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51fd734d214a528a503c45b5a9650a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d732e4aca90d4327926e1cfc64325a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3601b70bdf43c38b1c171a18dd2749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged model pushed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa62c03427145d6b43f1f3d1c661149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eafaf2b18174717a2f27b0d766537ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8d76830a184da287e893c32c66ef42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer pushed!\n",
      "\n",
      "üîó View at: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct\n",
      "\n",
      "==================================================\n",
      "‚ö†Ô∏è IMPORTANT: LOADING INSTRUCTIONS\n",
      "==================================================\n",
      "For BEST accuracy (~97%), load WITHOUT quantization:\n",
      "```python\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    \"Eng-Elias/qwen3-0.6b-terminal-instruct\",\n",
      "    torch_dtype=torch.float16,\n",
      "    device_map=\"auto\"\n",
      ")\n",
      "```\n",
      "\n",
      "With 4-bit quantization, expect ~80-83% accuracy due to\n",
      "quantization mismatch from QLoRA training.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ PUBLISHING MERGED MODEL TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_MERGED}\")\n",
    "\n",
    "try:\n",
    "    # Push merged model to HuggingFace\n",
    "    merged_model.push_to_hub(\n",
    "        HF_REPO_MERGED,\n",
    "        commit_message=\"Upload merged model for terminal command generation\",\n",
    "        private=False\n",
    "    )\n",
    "    print(\"‚úÖ Merged model pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    tokenizer.push_to_hub(\n",
    "        HF_REPO_MERGED,\n",
    "        commit_message=\"Upload tokenizer\"\n",
    "    )\n",
    "    print(\"‚úÖ Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nüîó View at: https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "    \n",
    "    # Important usage note\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚ö†Ô∏è IMPORTANT: LOADING INSTRUCTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"For BEST accuracy (~97%), load WITHOUT quantization:\")\n",
    "    print(\"```python\")\n",
    "    print(\"model = AutoModelForCausalLM.from_pretrained(\")\n",
    "    print(f'    \"{HF_REPO_MERGED}\",')\n",
    "    print(\"    torch_dtype=torch.float16,\")\n",
    "    print(\"    device_map=\\\"auto\\\"\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    print(\"\\nWith 4-bit quantization, expect ~80-83% accuracy due to\")\n",
    "    print(\"quantization mismatch from QLoRA training.\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to push merged model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ ALL TASKS COMPLETED!\n",
      "============================================================\n",
      "\n",
      "üì¶ LOCAL SAVES:\n",
      "   LoRA Adapters: ../outputs/lora_adapters\n",
      "   Merged Model: ../outputs/merged_model\n",
      "\n",
      "üåê HUGGINGFACE REPOS:\n",
      "   Adapters: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
      "   Merged:   https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct\n",
      "\n",
      "üìä EVALUATION RESULTS:\n",
      "   In-Memory (LoRA + 4-bit base): 96.0%\n",
      "   Merged Model (float16):        62.0%\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è QUANTIZATION MISMATCH - IMPORTANT NOTES\n",
      "============================================================\n",
      "\n",
      "During QLoRA training, LoRA adapters learn to compensate for the specific\n",
      "quantization errors of the 4-bit base model. This creates a mismatch when\n",
      "loading models differently:\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Loading Method                          ‚îÇ Expected Accuracy ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Base (4-bit) + LoRA adapters            ‚îÇ ~97% (best)       ‚îÇ\n",
      "‚îÇ Merged model (float16, no quantization) ‚îÇ ~97% (best)       ‚îÇ\n",
      "‚îÇ Merged model (re-quantized to 4-bit)    ‚îÇ ~80-83% (degraded)‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "1. For production with limited VRAM: Use base model + LoRA adapters\n",
      "2. For production with enough VRAM: Use merged model in float16\n",
      "3. Avoid re-quantizing the merged model (causes accuracy loss)\n",
      "\n",
      "üìå NEXT STEPS:\n",
      "   1. Run 02_evaluate_all_sources.ipynb to compare all loading methods\n",
      "   2. Run 03_load_and_test_all.ipynb for interactive testing\n",
      "   3. Update model cards on HuggingFace with loading instructions\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ALL TASKS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüì¶ LOCAL SAVES:\")\n",
    "print(f\"   LoRA Adapters: {CONFIG['adapter_save_path']}\")\n",
    "print(f\"   Merged Model: {CONFIG['merged_model_path']}\")\n",
    "\n",
    "print(\"\\nüåê HUGGINGFACE REPOS:\")\n",
    "print(f\"   Adapters: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "print(f\"   Merged:   https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "\n",
    "print(\"\\nüìä EVALUATION RESULTS:\")\n",
    "print(f\"   In-Memory (LoRA + 4-bit base): {100*single_results['exact_match']/single_results['total']:.1f}%\")\n",
    "print(f\"   Merged Model (float16):        {merged_accuracy:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ö†Ô∏è QUANTIZATION MISMATCH - IMPORTANT NOTES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "During QLoRA training, LoRA adapters learn to compensate for the specific\n",
    "quantization errors of the 4-bit base model. This creates a mismatch when\n",
    "loading models differently:\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Loading Method                          ‚îÇ Expected Accuracy ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Base (4-bit) + LoRA adapters            ‚îÇ ~97% (best)       ‚îÇ\n",
    "‚îÇ Merged model (float16, no quantization) ‚îÇ ~97% (best)       ‚îÇ\n",
    "‚îÇ Merged model (re-quantized to 4-bit)    ‚îÇ ~80-83% (degraded)‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "1. For production with limited VRAM: Use base model + LoRA adapters\n",
    "2. For production with enough VRAM: Use merged model in float16\n",
    "3. Avoid re-quantizing the merged model (causes accuracy loss)\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìå NEXT STEPS:\")\n",
    "print(\"   1. Run 02_evaluate_all_sources.ipynb to compare all loading methods\")\n",
    "print(\"   2. Run 03_load_and_test_all.ipynb for interactive testing\")\n",
    "print(\"   3. Update model cards on HuggingFace with loading instructions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
