{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Evaluate, and Publish Terminal Command Model\n",
    "\n",
    "This notebook:\n",
    "1. Trains Qwen3-0.6B with QLoRA on terminal command dataset\n",
    "2. Evaluates the model while still in memory\n",
    "3. Saves LoRA adapters locally and publishes to HuggingFace\n",
    "4. Saves merged model locally and publishes to HuggingFace\n",
    "\n",
    "**HuggingFace Repos:**\n",
    "- Adapters: `{username}/qwen3-0.6b-terminal-instruct-lora`\n",
    "- Merged: `{username}/qwen3-0.6b-terminal-instruct`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup and CUDA Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CUDA VERIFICATION\n",
      "==================================================\n",
      "PyTorch Version: 2.9.0+cu130\n",
      "‚úÖ CUDA is available!\n",
      "   GPU: NVIDIA GeForce RTX 2060\n",
      "   VRAM: 6.00 GB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CUDA VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA is available!\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"‚ùå CUDA is NOT available!\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from huggingface_hub import HfApi, login\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CONFIGURATION\n",
      "==================================================\n",
      "HuggingFace Username: Eng-Elias\n",
      "Adapters Repo: Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
      "Merged Model Repo: Eng-Elias/qwen3-0.6b-terminal-instruct\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# HUGGINGFACE CONFIGURATION\n",
    "# ============================================\n",
    "HF_USERNAME = \"Eng-Elias\"  # <-- Change this to your HuggingFace username\n",
    "HF_TOKEN = None  # Will prompt if None, or set your token here\n",
    "\n",
    "# Repository names\n",
    "HF_REPO_ADAPTERS = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct-lora\"\n",
    "HF_REPO_MERGED = f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct\"\n",
    "\n",
    "# ============================================\n",
    "# MODEL & TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \n",
    "    # Paths\n",
    "    \"train_data\": \"../dataset/generated/processed/train.json\",\n",
    "    \"dev_data\": \"../dataset/generated/processed/dev.json\",\n",
    "    \"test_data\": \"../dataset/generated/processed/test.json\",\n",
    "    \"output_dir\": \"../outputs/terminal_command_model\",\n",
    "    \"adapter_save_path\": \"../outputs/lora_adapters\",\n",
    "    \"merged_model_path\": \"../outputs/merged_model\",\n",
    "    \n",
    "    # Quantization (for 6GB VRAM)\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \n",
    "    # Training Configuration - RTX 2060 Optimized\n",
    "    \"num_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_seq_length\": 256,\n",
    "    \"logging_steps\": 25,\n",
    "    \"eval_steps\": 100,\n",
    "    \"save_steps\": 200,\n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    # Memory Optimizations\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \n",
    "    # Evaluation\n",
    "    \"max_new_tokens\": 150,\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"HuggingFace Username: {HF_USERNAME}\")\n",
    "print(f\"Adapters Repo: {HF_REPO_ADAPTERS}\")\n",
    "print(f\"Merged Model Repo: {HF_REPO_MERGED}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your HuggingFace token (get it from https://huggingface.co/settings/tokens):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in to HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace\n",
    "if HF_TOKEN is None:\n",
    "    print(\"Please enter your HuggingFace token (get it from https://huggingface.co/settings/tokens):\")\n",
    "    HF_TOKEN = input(\"Token: \")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "‚úÖ Train samples: 9780\n",
      "‚úÖ Eval samples: 1150\n",
      "‚úÖ Test samples: 577\n",
      "\n",
      "üìù Sample data:\n",
      "{\n",
      "  \"instruction\": \"Find all .css files in temp\",\n",
      "  \"input\": \"[LINUX]\",\n",
      "  \"output\": \"find temp -name '*.css'\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def load_json_dataset(filepath):\n",
    "    \"\"\"Load dataset from JSON file.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = load_json_dataset(CONFIG[\"train_data\"])\n",
    "eval_dataset = load_json_dataset(CONFIG[\"dev_data\"])\n",
    "test_dataset = load_json_dataset(CONFIG[\"test_data\"])\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\nüìù Sample data:\")\n",
    "print(json.dumps(train_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "‚úÖ Tokenizer loaded\n",
      "\n",
      "Configuring 4-bit quantization...\n",
      "Loading model with quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: Qwen/Qwen3-0.6B\n",
      "   VRAM used: 0.50 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Quantization config\n",
    "print(\"\\nConfiguring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CONFIG[\"load_in_4bit\"],\n",
    "    bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_use_double_quant=CONFIG[\"bnb_4bit_use_double_quant\"],\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Loading model with quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"base_model\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {CONFIG['base_model']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   VRAM used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for LoRA training...\n",
      "\n",
      "==================================================\n",
      "TRAINABLE PARAMETERS\n",
      "==================================================\n",
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7638\n",
      "\n",
      "VRAM used after LoRA setup: 0.81 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=CONFIG[\"gradient_checkpointing\"]\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRAINABLE PARAMETERS\")\n",
    "print(\"=\" * 50)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nVRAM used after LoRA setup: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sample formatted prompt:\n",
      "--------------------------------------------------\n",
      "### Instruction:\n",
      "Find all .css files in temp\n",
      "\n",
      "### Input:\n",
      "[LINUX]\n",
      "\n",
      "### Response:\n",
      "find temp -name '*.css'<|im_end|>\n",
      "--------------------------------------------------\n",
      "\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a6257d24e24625a5a49740baa1dba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/9780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d27169c47445d4accaa4ed57529038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval:   0%|          | 0/1150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized train: 9780 samples\n",
      "‚úÖ Tokenized eval: 1150 samples\n"
     ]
    }
   ],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format sample into instruction prompt with EOS token.\"\"\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    input_text = sample.get(\"input\", \"\")\n",
    "    output = sample[\"output\"]\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}{tokenizer.eos_token}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize and prepare data for training.\"\"\"\n",
    "    prompts = [format_instruction({\"instruction\": inst, \"input\": inp, \"output\": out}) \n",
    "               for inst, inp, out in zip(examples[\"instruction\"], \n",
    "                                          examples[\"input\"], \n",
    "                                          examples[\"output\"])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_seq_length\"],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"üìù Sample formatted prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(format_instruction(train_dataset[0]))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing eval\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenized train: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úÖ Tokenized eval: {len(tokenized_eval)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Training Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized\n",
      "   Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"adapter_save_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"merged_model_path\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    optim=CONFIG[\"optim\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print(f\"   Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ STARTING TRAINING\n",
      "==================================================\n",
      "Start time: 2025-12-29 22:37:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1836/1836 3:49:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.310800</td>\n",
       "      <td>0.237161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.131519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.107887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.092697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.080882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>0.070850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.064213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.059128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.054755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.050887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.047771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.045874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.044413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>0.043479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.042553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.042051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.041806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.041704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úÖ TRAINING COMPLETED!\n",
      "==================================================\n",
      "End time: 2025-12-30 02:26:42\n",
      "Training time: 229.17 minutes\n",
      "Training loss: 0.3007\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Training time: {train_result.metrics['train_runtime'] / 60:.2f} minutes\")\n",
    "    print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Evaluate Model In-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üîç EVALUATING MODEL IN-MEMORY\n",
      "==================================================\n",
      "\n",
      "üìä Evaluating Single OS Commands...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:27<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä IN-MEMORY EVALUATION RESULTS\n",
      "==================================================\n",
      "Total evaluated: 100\n",
      "Exact match: 93 (93.0%)\n",
      "Fuzzy match: 94 (94.0%)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_command(instruction, input_text=\"\", max_new_tokens=150):\n",
    "    \"\"\"Generate command from instruction.\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=200).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    response = response.split(\"### \")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return pred.strip() == gold.strip()\n",
    "\n",
    "def fuzzy_match(pred, gold):\n",
    "    pred_norm = ' '.join(pred.lower().split())\n",
    "    gold_norm = ' '.join(gold.lower().split())\n",
    "    return pred_norm == gold_norm or gold_norm in pred_norm or pred_norm in gold_norm\n",
    "\n",
    "# Load test data\n",
    "with open(CONFIG[\"test_data\"], 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "single_os_tests = [t for t in test_data if t[\"input\"] in [\"[LINUX]\", \"[WINDOWS]\", \"[MAC]\", \"\"]]\n",
    "json_tests = [t for t in test_data if \"JSON\" in t[\"input\"].upper()]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üîç EVALUATING MODEL IN-MEMORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Evaluate single OS commands\n",
    "print(\"\\nüìä Evaluating Single OS Commands...\")\n",
    "single_results = {\"total\": 0, \"exact_match\": 0, \"fuzzy_match\": 0}\n",
    "\n",
    "sample_size = min(100, len(single_os_tests))  # Sample for speed\n",
    "for sample in tqdm(single_os_tests[:sample_size], desc=\"Evaluating\"):\n",
    "    pred = generate_command(sample[\"instruction\"], sample[\"input\"])\n",
    "    gold = sample[\"output\"]\n",
    "    \n",
    "    single_results[\"total\"] += 1\n",
    "    if exact_match(pred, gold):\n",
    "        single_results[\"exact_match\"] += 1\n",
    "    if fuzzy_match(pred, gold):\n",
    "        single_results[\"fuzzy_match\"] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìä IN-MEMORY EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total evaluated: {single_results['total']}\")\n",
    "print(f\"Exact match: {single_results['exact_match']} ({100*single_results['exact_match']/single_results['total']:.1f}%)\")\n",
    "print(f\"Fuzzy match: {single_results['fuzzy_match']} ({100*single_results['fuzzy_match']/single_results['total']:.1f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save LoRA Adapters Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üíæ SAVING LORA ADAPTERS LOCALLY\n",
      "==================================================\n",
      "‚úÖ LoRA adapters saved to: ../outputs/lora_adapters\n",
      "‚úÖ Tokenizer saved to: ../outputs/lora_adapters\n",
      "‚úÖ Config saved to: ../outputs/lora_adapters/training_config.json\n",
      "\n",
      "üìÅ Saved adapter files:\n",
      "   adapter_config.json: 0.00 MB\n",
      "   adapter_model.safetensors: 17.53 MB\n",
      "   added_tokens.json: 0.00 MB\n",
      "   chat_template.jinja: 0.00 MB\n",
      "   merges.txt: 1.59 MB\n",
      "   README.md: 0.00 MB\n",
      "   special_tokens_map.json: 0.00 MB\n",
      "   tokenizer.json: 10.89 MB\n",
      "   tokenizer_config.json: 0.01 MB\n",
      "   training_config.json: 0.00 MB\n",
      "   vocab.json: 2.65 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üíæ SAVING LORA ADAPTERS LOCALLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "adapter_path = CONFIG[\"adapter_save_path\"]\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(adapter_path)\n",
    "print(f\"‚úÖ LoRA adapters saved to: {adapter_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"‚úÖ Tokenizer saved to: {adapter_path}\")\n",
    "\n",
    "# Save training config\n",
    "config_save_path = f\"{adapter_path}/training_config.json\"\n",
    "with open(config_save_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"‚úÖ Config saved to: {config_save_path}\")\n",
    "\n",
    "print(\"\\nüìÅ Saved adapter files:\")\n",
    "for f in Path(adapter_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Publish LoRA Adapters to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ PUBLISHING LORA ADAPTERS TO HUGGINGFACE\n",
      "==================================================\n",
      "Repository: Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67d695622654874b79d740522b33495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa174d43da4417a98d8248e4dbce291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06eaab1143884022bb2b5e1bcecd096c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters pushed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799c1a9cd3fc4d11855f9fd128d1f20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f752d79848461081fd7cc4daf56749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb185c1c48ce4cdaa4cf5a020aa7f74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer pushed!\n",
      "\n",
      "üîó View at: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ PUBLISHING LORA ADAPTERS TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_ADAPTERS}\")\n",
    "\n",
    "try:\n",
    "    # Push adapters to HuggingFace\n",
    "    model.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload LoRA adapters for terminal command generation\",\n",
    "        private=False  # Set to True if you want private repo\n",
    "    )\n",
    "    print(\"‚úÖ LoRA adapters pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    tokenizer.push_to_hub(\n",
    "        HF_REPO_ADAPTERS,\n",
    "        commit_message=\"Upload tokenizer\"\n",
    "    )\n",
    "    print(\"‚úÖ Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nüîó View at: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to push adapters: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Create and Save Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üîÄ PREPARING MERGED MODEL\n",
      "==================================================\n",
      "Step 1: Verifying trained model is still in memory...\n",
      "   Model type: PeftModelForCausalLM\n",
      "\n",
      "Step 2: Quick accuracy verification before merge...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:13<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Pre-merge accuracy (trained PeftModel): 90.0%\n",
      "\n",
      "Step 3: Merging LoRA adapters into base model...\n",
      "   Calling model.merge_and_unload() on trained model...\n",
      "‚úÖ Adapters merged successfully!\n",
      "\n",
      "Step 4: Saving merged model locally...\n",
      "‚úÖ Merged model saved to: ../outputs/merged_model\n",
      "\n",
      "üìÅ Merged model files:\n",
      "   added_tokens.json: 0.00 MB\n",
      "   chat_template.jinja: 0.00 MB\n",
      "   config.json: 0.00 MB\n",
      "   generation_config.json: 0.00 MB\n",
      "   merges.txt: 1.59 MB\n",
      "   model.safetensors: 810.80 MB\n",
      "   special_tokens_map.json: 0.00 MB\n",
      "   tokenizer.json: 10.89 MB\n",
      "   tokenizer_config.json: 0.01 MB\n",
      "   vocab.json: 2.65 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üîÄ PREPARING MERGED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ============================================\n",
    "# IMPORTANT: Merge directly from trained model in memory\n",
    "# ============================================\n",
    "# The trained PeftModel (model) is still in memory from training.\n",
    "# We merge directly from it - do NOT reload the base model separately.\n",
    "# This preserves the exact training state and maintains accuracy.\n",
    "# ============================================\n",
    "\n",
    "print(\"Step 1: Verifying trained model is still in memory...\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "\n",
    "# Verify accuracy before merge (quick check)\n",
    "print(\"\\nStep 2: Quick accuracy verification before merge...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "verify_results = {\"total\": 0, \"exact_match\": 0}\n",
    "verification_size = min(50, len(single_os_tests))\n",
    "\n",
    "for sample in tqdm(single_os_tests[:verification_size], desc=\"Verifying\"):\n",
    "    pred = generate_command(sample[\"instruction\"], sample[\"input\"])\n",
    "    gold = sample[\"output\"]\n",
    "    verify_results[\"total\"] += 1\n",
    "    if pred.strip() == gold.strip():\n",
    "        verify_results[\"exact_match\"] += 1\n",
    "\n",
    "pre_merge_accuracy = 100 * verify_results[\"exact_match\"] / verify_results[\"total\"]\n",
    "print(f\"\\n‚úÖ Pre-merge accuracy (trained PeftModel): {pre_merge_accuracy:.1f}%\")\n",
    "\n",
    "if pre_merge_accuracy < 90:\n",
    "    print(\"   ‚ö†Ô∏è Warning: Accuracy seems low, but proceeding with merge...\")\n",
    "\n",
    "# Step 3: Merge adapters into base model\n",
    "print(\"\\nStep 3: Merging LoRA adapters into base model...\")\n",
    "print(\"   Calling model.merge_and_unload() on trained model...\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "print(\"‚úÖ Adapters merged successfully!\")\n",
    "\n",
    "# Step 4: Save merged model locally\n",
    "print(\"\\nStep 4: Saving merged model locally...\")\n",
    "merged_model_path = CONFIG[\"merged_model_path\"]\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(f\"‚úÖ Merged model saved to: {merged_model_path}\")\n",
    "\n",
    "print(\"\\nüìÅ Merged model files:\")\n",
    "for f in Path(merged_model_path).iterdir():\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"   {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Publish Merged Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üöÄ PUBLISHING MERGED MODEL TO HUGGINGFACE\n",
      "==================================================\n",
      "Repository: Eng-Elias/qwen3-0.6b-terminal-instruct\n",
      "(LoRA adapters already pushed in Cell 13)\n",
      "\n",
      "üì§ Pushing merged model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e64502484a94eb498f7feabbf8752fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f542a3579f1424db95c5491c0ddcd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged model pushed!\n",
      "\n",
      "üì§ Pushing tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd6be0ce8f04f259f215b81708a09b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1c83aca5e94b42bb50f42425809682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8e776d9fe94952bba2cfa9932e94db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer pushed!\n",
      "\n",
      "üîó View at: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"üöÄ PUBLISHING MERGED MODEL TO HUGGINGFACE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Repository: {HF_REPO_MERGED}\")\n",
    "print(\"(LoRA adapters already pushed in Cell 13)\")\n",
    "\n",
    "try:\n",
    "    # Push merged model\n",
    "    print(f\"\\nüì§ Pushing merged model...\")\n",
    "    merged_model.push_to_hub(\n",
    "        HF_REPO_MERGED,\n",
    "        commit_message=\"Upload merged terminal command model\",\n",
    "        private=False\n",
    "    )\n",
    "    print(f\"‚úÖ Merged model pushed!\")\n",
    "    \n",
    "    # Push tokenizer\n",
    "    print(\"\\nüì§ Pushing tokenizer...\")\n",
    "    tokenizer.push_to_hub(HF_REPO_MERGED)\n",
    "    print(\"‚úÖ Tokenizer pushed!\")\n",
    "    \n",
    "    print(f\"\\nüîó View at: https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error pushing to HuggingFace: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nMake sure you're logged in with: huggingface-cli login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ ALL TASKS COMPLETED!\n",
      "============================================================\n",
      "\n",
      "üì¶ LOCAL SAVES:\n",
      "   LoRA Adapters: ../outputs/lora_adapters\n",
      "   Merged Model:  ../outputs/merged_model\n",
      "\n",
      "üåê HUGGINGFACE REPOS:\n",
      "   Adapters: https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
      "   Merged:   https://huggingface.co/Eng-Elias/qwen3-0.6b-terminal-instruct\n",
      "\n",
      "üìä ACCURACY RESULTS:\n",
      "   In-Memory (after training): 93.0%\n",
      "   Pre-Merge Verification:     90.0%\n",
      "\n",
      "============================================================\n",
      "üìã HOW TO LOAD THE MODELS\n",
      "============================================================\n",
      "\n",
      "# Option 1: Load merged model directly (simplest)\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\"Eng-Elias/qwen3-0.6b-terminal-instruct\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Eng-Elias/qwen3-0.6b-terminal-instruct\")\n",
      "\n",
      "# Option 2: Load base model + LoRA adapters  \n",
      "from peft import PeftModel\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "base = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
      "model = PeftModel.from_pretrained(base, \"Eng-Elias/qwen3-0.6b-terminal-instruct-lora\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Eng-Elias/qwen3-0.6b-terminal-instruct-lora\")\n",
      "\n",
      "üìå NEXT STEPS:\n",
      "   1. Run 02_evaluate_all_sources.ipynb to verify accuracy from all sources\n",
      "   2. Run 03_load_and_test_all.ipynb for interactive testing\n",
      "   3. Update model cards on HuggingFace\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ALL TASKS COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüì¶ LOCAL SAVES:\")\n",
    "print(f\"   LoRA Adapters: {CONFIG['adapter_save_path']}\")\n",
    "print(f\"   Merged Model:  {CONFIG['merged_model_path']}\")\n",
    "\n",
    "print(\"\\nüåê HUGGINGFACE REPOS:\")\n",
    "print(f\"   Adapters: https://huggingface.co/{HF_REPO_ADAPTERS}\")\n",
    "print(f\"   Merged:   https://huggingface.co/{HF_REPO_MERGED}\")\n",
    "\n",
    "print(\"\\nüìä ACCURACY RESULTS:\")\n",
    "print(f\"   In-Memory (after training): {100*single_results['exact_match']/single_results['total']:.1f}%\")\n",
    "print(f\"   Pre-Merge Verification:     {pre_merge_accuracy:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã HOW TO LOAD THE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "# Option 1: Load merged model directly (simplest)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{}\")\n",
    "\n",
    "# Option 2: Load base model + LoRA adapters  \n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "model = PeftModel.from_pretrained(base, \"{}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{}\")\n",
    "\"\"\".format(HF_REPO_MERGED, HF_REPO_MERGED, HF_REPO_ADAPTERS, HF_REPO_ADAPTERS))\n",
    "\n",
    "print(\"üìå NEXT STEPS:\")\n",
    "print(\"   1. Run 02_evaluate_all_sources.ipynb to verify accuracy from all sources\")\n",
    "print(\"   2. Run 03_load_and_test_all.ipynb for interactive testing\")\n",
    "print(\"   3. Update model cards on HuggingFace\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
