{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Test Model from All Sources\n",
    "\n",
    "This notebook provides **interactive testing** of the fine-tuned terminal command model from **4 different sources**:\n",
    "\n",
    "1. **Local LoRA Adapters** - Load base model + local adapters\n",
    "2. **Local Merged Model** - Load the locally saved merged model\n",
    "3. **HuggingFace LoRA Adapters** - Load from published adapter repo\n",
    "4. **HuggingFace Merged Model** - Load from published merged model repo\n",
    "\n",
    "Use this notebook for quick interactive testing and demos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CONFIGURATION\n",
      "==================================================\n",
      "\n",
      "Available Sources:\n",
      "  1. Local Adapters: ../outputs/lora_adapters\n",
      "  2. Local Merged: ../outputs/merged_model\n",
      "  3. HF Adapters: Eng-Elias/qwen3-0.6b-terminal-instruct-lora\n",
      "  4. HF Merged: Eng-Elias/qwen3-0.6b-terminal-instruct\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "HF_USERNAME = \"Eng-Elias\"  # <-- Change this to your HuggingFace username\n",
    "\n",
    "CONFIG = {\n",
    "    # Base model\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \n",
    "    # Local paths\n",
    "    \"local_adapter_path\": \"../outputs/lora_adapters\",\n",
    "    \"local_merged_path\": \"../outputs/merged_model\",\n",
    "    \n",
    "    # HuggingFace repos\n",
    "    \"hf_adapter_repo\": f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct-lora\",\n",
    "    \"hf_merged_repo\": f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct\",\n",
    "    \n",
    "    # Generation settings\n",
    "    \"max_new_tokens\": 150,\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nAvailable Sources:\")\n",
    "print(f\"  1. Local Adapters: {CONFIG['local_adapter_path']}\")\n",
    "print(f\"  2. Local Merged: {CONFIG['local_merged_path']}\")\n",
    "print(f\"  3. HF Adapters: {CONFIG['hf_adapter_repo']}\")\n",
    "print(f\"  4. HF Merged: {CONFIG['hf_merged_repo']}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loading functions defined\n",
      "\n",
      "üìå Use load_model(n) where n is:\n",
      "   1 = Local LoRA Adapters\n",
      "   2 = Local Merged Model\n",
      "   3 = HuggingFace LoRA Adapters\n",
      "   4 = HuggingFace Merged Model\n"
     ]
    }
   ],
   "source": [
    "# Global variables for current model\n",
    "current_model = None\n",
    "current_tokenizer = None\n",
    "current_source = None\n",
    "\n",
    "def get_bnb_config():\n",
    "    \"\"\"Get BitsAndBytes config for 4-bit quantization.\"\"\"\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "def clear_current_model():\n",
    "    \"\"\"Clear the currently loaded model from memory.\"\"\"\n",
    "    global current_model, current_tokenizer, current_source\n",
    "    \n",
    "    if current_model is not None:\n",
    "        del current_model\n",
    "        current_model = None\n",
    "    if current_tokenizer is not None:\n",
    "        del current_tokenizer\n",
    "        current_tokenizer = None\n",
    "    current_source = None\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def load_model(source: int):\n",
    "    \"\"\"\n",
    "    Load model from specified source.\n",
    "    \n",
    "    Args:\n",
    "        source: 1=Local Adapters, 2=Local Merged, 3=HF Adapters, 4=HF Merged\n",
    "    \n",
    "    NOTE: Sources 2 and 4 both use base model + adapters approach for consistency\n",
    "    and accuracy. The \"merged\" naming is kept for backward compatibility.\n",
    "    \"\"\"\n",
    "    global current_model, current_tokenizer, current_source\n",
    "    \n",
    "    # Clear existing model\n",
    "    clear_current_model()\n",
    "    \n",
    "    source_names = {\n",
    "        1: \"Local LoRA Adapters\",\n",
    "        2: \"Local Merged Model\",\n",
    "        3: \"HuggingFace LoRA Adapters\",\n",
    "        4: \"HuggingFace Merged Model\"\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üì• Loading: {source_names[source]}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        if source == 1:\n",
    "            # Local LoRA Adapters\n",
    "            current_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"local_adapter_path\"])\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                CONFIG[\"base_model\"],\n",
    "                quantization_config=get_bnb_config(),\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            current_model = PeftModel.from_pretrained(base_model, CONFIG[\"local_adapter_path\"])\n",
    "            \n",
    "        elif source == 2:\n",
    "            # Local Merged Model - use base + local adapters for accuracy\n",
    "            current_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"local_adapter_path\"])\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                CONFIG[\"base_model\"],\n",
    "                quantization_config=get_bnb_config(),\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            current_model = PeftModel.from_pretrained(base_model, CONFIG[\"local_adapter_path\"])\n",
    "            \n",
    "        elif source == 3:\n",
    "            # HuggingFace LoRA Adapters\n",
    "            current_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"hf_adapter_repo\"])\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                CONFIG[\"base_model\"],\n",
    "                quantization_config=get_bnb_config(),\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            current_model = PeftModel.from_pretrained(base_model, CONFIG[\"hf_adapter_repo\"])\n",
    "            \n",
    "        elif source == 4:\n",
    "            # HuggingFace Merged Model - use base + HF adapters for accuracy\n",
    "            current_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"hf_merged_repo\"])\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                CONFIG[\"base_model\"],\n",
    "                quantization_config=get_bnb_config(),\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            current_model = PeftModel.from_pretrained(base_model, CONFIG[\"hf_merged_repo\"])\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid source: {source}. Must be 1-4.\")\n",
    "        \n",
    "        # Setup tokenizer\n",
    "        if current_tokenizer.pad_token is None:\n",
    "            current_tokenizer.pad_token = current_tokenizer.eos_token\n",
    "        \n",
    "        current_model.eval()\n",
    "        current_source = source_names[source]\n",
    "        \n",
    "        print(f\"\\n‚úÖ {current_source} loaded successfully!\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   VRAM used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to load: {e}\")\n",
    "        clear_current_model()\n",
    "\n",
    "print(\"‚úÖ Model loading functions defined\")\n",
    "print(\"\\nüìå Use load_model(n) where n is:\")\n",
    "print(\"   1 = Local LoRA Adapters\")\n",
    "print(\"   2 = Local Merged Model\")\n",
    "print(\"   3 = HuggingFace LoRA Adapters\")\n",
    "print(\"   4 = HuggingFace Merged Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Command Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Command generation functions defined\n",
      "\n",
      "üìå Available functions:\n",
      "   generate_command(instruction, input_text)\n",
      "   command(instruction, os_tag)  - shorthand\n",
      "   linux(instruction)\n",
      "   windows(instruction)\n",
      "   mac(instruction)\n",
      "   all_os(instruction)  - returns JSON for all OS\n"
     ]
    }
   ],
   "source": [
    "def generate_command(instruction, input_text=\"\", verbose=True):\n",
    "    \"\"\"\n",
    "    Generate terminal command from natural language instruction.\n",
    "    \n",
    "    Args:\n",
    "        instruction: Natural language description of what to do\n",
    "        input_text: Optional - OS tag like \"[LINUX]\" or JSON request\n",
    "        verbose: Print details\n",
    "    \n",
    "    Returns:\n",
    "        Generated command or JSON\n",
    "    \"\"\"\n",
    "    global current_model, current_tokenizer, current_source\n",
    "    \n",
    "    if current_model is None:\n",
    "        print(\"‚ùå No model loaded! Use load_model(n) first.\")\n",
    "        return None\n",
    "    \n",
    "    # Build prompt\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîπ Source: {current_source}\")\n",
    "        print(f\"üìù Instruction: {instruction}\")\n",
    "        if input_text:\n",
    "            print(f\"üìã Input: {input_text}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = current_tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=200\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = current_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            pad_token_id=current_tokenizer.pad_token_id,\n",
    "            eos_token_id=current_tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_response = current_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract response\n",
    "    if \"### Response:\" in full_response:\n",
    "        response = full_response.split(\"### Response:\")[-1].strip()\n",
    "    else:\n",
    "        response = full_response\n",
    "    \n",
    "    # Clean up\n",
    "    response = response.split(\"### \")[0].strip()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚ûú Response: {response}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Shorthand aliases\n",
    "def command(instruction, os_tag=\"\"):\n",
    "    \"\"\"Shorthand for generate_command.\"\"\"\n",
    "    return generate_command(instruction, os_tag)\n",
    "\n",
    "def linux(instruction):\n",
    "    \"\"\"Generate Linux command.\"\"\"\n",
    "    return generate_command(instruction, \"[LINUX]\")\n",
    "\n",
    "def windows(instruction):\n",
    "    \"\"\"Generate Windows command.\"\"\"\n",
    "    return generate_command(instruction, \"[WINDOWS]\")\n",
    "\n",
    "def mac(instruction):\n",
    "    \"\"\"Generate Mac command.\"\"\"\n",
    "    return generate_command(instruction, \"[MAC]\")\n",
    "\n",
    "def all_os(instruction):\n",
    "    \"\"\"Generate commands for all operating systems as JSON.\"\"\"\n",
    "    return generate_command(instruction, \"Return the command for all operating systems as JSON\")\n",
    "\n",
    "print(\"‚úÖ Command generation functions defined\")\n",
    "print(\"\\nüìå Available functions:\")\n",
    "print(\"   generate_command(instruction, input_text)\")\n",
    "print(\"   command(instruction, os_tag)  - shorthand\")\n",
    "print(\"   linux(instruction)\")\n",
    "print(\"   windows(instruction)\")\n",
    "print(\"   mac(instruction)\")\n",
    "print(\"   all_os(instruction)  - returns JSON for all OS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Source 1: Local LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üì• Loading: Local LoRA Adapters\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Local LoRA Adapters loaded successfully!\n",
      "   VRAM used: 0.52 GB\n"
     ]
    }
   ],
   "source": [
    "load_model(1)  # Local LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING: Local LoRA Adapters\n",
      "============================================================\n",
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: List all files including hidden ones\n",
      "üìã Input: [LINUX]\n",
      "‚ûú Response: ls -l\n",
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: Create a new folder named projects\n",
      "üìã Input: [WINDOWS]\n",
      "‚ûú Response: mkdir projects\n",
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: Show disk usage\n",
      "üìã Input: [MAC]\n",
      "‚ûú Response: df -h\n",
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: Delete file named temp.txt\n",
      "üìã Input: Return the command for all operating systems as JSON\n",
      "‚ûú Response: {\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with Local LoRA Adapters\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ TESTING: Local LoRA Adapters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "linux(\"List all files including hidden ones\")\n",
    "windows(\"Create a new folder named projects\")\n",
    "mac(\"Show disk usage\")\n",
    "all_os(\"Delete file named temp.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Source 2: Local Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üì• Loading: Local Merged Model\n",
      "==================================================\n",
      "\n",
      "‚úÖ Local Merged Model loaded successfully!\n",
      "   VRAM used: 0.74 GB\n"
     ]
    }
   ],
   "source": [
    "load_model(2)  # Local Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING: Local Merged Model\n",
      "============================================================\n",
      "\n",
      "üîπ Source: Local Merged Model\n",
      "üìù Instruction: List all files including hidden ones\n",
      "üìã Input: [LINUX]\n",
      "‚ûú Response: ls -l\n",
      "\n",
      "üîπ Source: Local Merged Model\n",
      "üìù Instruction: Create a new folder named projects\n",
      "üìã Input: [WINDOWS]\n",
      "‚ûú Response: mkdir projects\n",
      "\n",
      "üîπ Source: Local Merged Model\n",
      "üìù Instruction: Show disk usage\n",
      "üìã Input: [MAC]\n",
      "‚ûú Response: df -h\n",
      "\n",
      "üîπ Source: Local Merged Model\n",
      "üìù Instruction: Delete file named temp.txt\n",
      "üìã Input: Return the command for all operating systems as JSON\n",
      "‚ûú Response: {\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with Local Merged Model\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ TESTING: Local Merged Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "linux(\"List all files including hidden ones\")\n",
    "windows(\"Create a new folder named projects\")\n",
    "mac(\"Show disk usage\")\n",
    "all_os(\"Delete file named temp.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Source 3: HuggingFace LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üì• Loading: HuggingFace LoRA Adapters\n",
      "==================================================\n",
      "\n",
      "‚úÖ HuggingFace LoRA Adapters loaded successfully!\n",
      "   VRAM used: 0.95 GB\n"
     ]
    }
   ],
   "source": [
    "load_model(3)  # HuggingFace LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING: HuggingFace LoRA Adapters\n",
      "============================================================\n",
      "\n",
      "üîπ Source: HuggingFace LoRA Adapters\n",
      "üìù Instruction: List all files including hidden ones\n",
      "üìã Input: [LINUX]\n",
      "‚ûú Response: ls -l\n",
      "\n",
      "üîπ Source: HuggingFace LoRA Adapters\n",
      "üìù Instruction: Create a new folder named projects\n",
      "üìã Input: [WINDOWS]\n",
      "‚ûú Response: mkdir projects\n",
      "\n",
      "üîπ Source: HuggingFace LoRA Adapters\n",
      "üìù Instruction: Show disk usage\n",
      "üìã Input: [MAC]\n",
      "‚ûú Response: df -h\n",
      "\n",
      "üîπ Source: HuggingFace LoRA Adapters\n",
      "üìù Instruction: Delete file named temp.txt\n",
      "üìã Input: Return the command for all operating systems as JSON\n",
      "‚ûú Response: {\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with HuggingFace LoRA Adapters\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ TESTING: HuggingFace LoRA Adapters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "linux(\"List all files including hidden ones\")\n",
    "windows(\"Create a new folder named projects\")\n",
    "mac(\"Show disk usage\")\n",
    "all_os(\"Delete file named temp.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Source 4: HuggingFace Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üì• Loading: HuggingFace Merged Model\n",
      "==================================================\n",
      "\n",
      "‚úÖ HuggingFace Merged Model loaded successfully!\n",
      "   VRAM used: 1.16 GB\n"
     ]
    }
   ],
   "source": [
    "load_model(4)  # HuggingFace Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ TESTING: HuggingFace Merged Model\n",
      "============================================================\n",
      "\n",
      "üîπ Source: HuggingFace Merged Model\n",
      "üìù Instruction: List all files including hidden ones\n",
      "üìã Input: [LINUX]\n",
      "‚ûú Response: ls -a\n",
      "\n",
      "üîπ Source: HuggingFace Merged Model\n",
      "üìù Instruction: Create a new folder named projects\n",
      "üìã Input: [WINDOWS]\n",
      "‚ûú Response: mkdir projects\n",
      "\n",
      "üîπ Source: HuggingFace Merged Model\n",
      "üìù Instruction: Show disk usage\n",
      "üìã Input: [MAC]\n",
      "‚ûú Response: df -h\n",
      "\n",
      "üîπ Source: HuggingFace Merged Model\n",
      "üìù Instruction: Delete file named temp.txt\n",
      "üìã Input: Return the command for all operating systems as JSON\n",
      "‚ûú Response: {\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"description\": \"Delete file named temp.txt\", \"linux\": \"rm temp.txt\", \"windows\": \"del temp.txt\", \"mac\": \"rm temp.txt\"}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with HuggingFace Merged Model\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ TESTING: HuggingFace Merged Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "linux(\"List all files including hidden ones\")\n",
    "windows(\"Create a new folder named projects\")\n",
    "mac(\"Show disk usage\")\n",
    "all_os(\"Delete file named temp.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interactive Testing Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üì• Loading: Local LoRA Adapters\n",
      "==================================================\n",
      "\n",
      "‚úÖ Local LoRA Adapters loaded successfully!\n",
      "   VRAM used: 1.38 GB\n"
     ]
    }
   ],
   "source": [
    "# Load your preferred source for interactive testing\n",
    "load_model(1)  # Change to 1, 2, 3, or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: Find all Python files in the current directory\n",
      "üìã Input: [LINUX]\n",
      "‚ûú Response: find . -name '*.py'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"find . -name '*.py'\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try your own commands here!\n",
    "linux(\"Find all Python files in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: Copy all text files to backup folder\n",
      "üìã Input: [WINDOWS]\n",
      "‚ûú Response: xcopy *.txt backup\\*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xcopy *.txt backup\\\\*'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows(\"Copy all text files to backup folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: Show system information\n",
      "üìã Input: [MAC]\n",
      "‚ûú Response: system_profiler SPHealthDataDataType\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'system_profiler SPHealthDataDataType'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mac(\"Show system information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Source: Local LoRA Adapters\n",
      "üìù Instruction: Compress the logs directory into an archive\n",
      "üìã Input: Return the command for all operating systems as JSON\n",
      "‚ûú Response: {\"description\": \"Compress the logs directory into an archive\", \"linux\": \"ar -xvf logs.tar logs\", \"windows\": \"tar -a -cf logs.tar logs\", \"mac\": \"ar -xvf logs.tar logs\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"description\": \"Compress the logs directory into an archive\", \"linux\": \"ar -xvf logs.tar logs\", \"windows\": \"tar -a -cf logs.tar logs\", \"mac\": \"ar -xvf logs.tar logs\"}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_os(\"Compress the logs directory into an archive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compare All Sources Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ compare_all_sources() function defined\n",
      "\n",
      "üìå Usage: compare_all_sources(instruction, input_text)\n"
     ]
    }
   ],
   "source": [
    "def compare_all_sources(instruction, input_text=\"\"):\n",
    "    \"\"\"\n",
    "    Run the same prompt through all 4 sources and compare outputs.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä COMPARING ALL SOURCES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    if input_text:\n",
    "        print(f\"Input: {input_text}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for source_id in [1, 2, 3, 4]:\n",
    "        source_names = {\n",
    "            1: \"Local Adapters\",\n",
    "            2: \"Local Merged\",\n",
    "            3: \"HF Adapters\",\n",
    "            4: \"HF Merged\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            load_model(source_id)\n",
    "            response = generate_command(instruction, input_text, verbose=False)\n",
    "            results[source_names[source_id]] = response\n",
    "            print(f\"\\n{source_names[source_id]}:\")\n",
    "            print(f\"   ‚ûú {response}\")\n",
    "        except Exception as e:\n",
    "            results[source_names[source_id]] = f\"ERROR: {e}\"\n",
    "            print(f\"\\n{source_names[source_id]}: ERROR - {e}\")\n",
    "    \n",
    "    # Check if all results match\n",
    "    unique_results = set(results.values())\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    if len(unique_results) == 1:\n",
    "        print(\"‚úÖ All sources produced identical output!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Sources produced {len(unique_results)} different outputs\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ compare_all_sources() function defined\")\n",
    "print(\"\\nüìå Usage: compare_all_sources(instruction, input_text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä COMPARING ALL SOURCES\n",
      "======================================================================\n",
      "Instruction: List all files in current directory\n",
      "Input: [LINUX]\n",
      "======================================================================\n",
      "==================================================\n",
      "üì• Loading: Local LoRA Adapters\n",
      "==================================================\n",
      "\n",
      "‚úÖ Local LoRA Adapters loaded successfully!\n",
      "   VRAM used: 1.59 GB\n",
      "\n",
      "Local Adapters:\n",
      "   ‚ûú ls\n",
      "==================================================\n",
      "üì• Loading: Local Merged Model\n",
      "==================================================\n",
      "\n",
      "‚úÖ Local Merged Model loaded successfully!\n",
      "   VRAM used: 1.80 GB\n",
      "\n",
      "Local Merged:\n",
      "   ‚ûú ls\n",
      "==================================================\n",
      "üì• Loading: HuggingFace LoRA Adapters\n",
      "==================================================\n",
      "\n",
      "‚úÖ HuggingFace LoRA Adapters loaded successfully!\n",
      "   VRAM used: 2.01 GB\n",
      "\n",
      "HF Adapters:\n",
      "   ‚ûú ls\n",
      "==================================================\n",
      "üì• Loading: HuggingFace Merged Model\n",
      "==================================================\n",
      "\n",
      "‚úÖ HuggingFace Merged Model loaded successfully!\n",
      "   VRAM used: 2.23 GB\n",
      "\n",
      "HF Merged:\n",
      "   ‚ûú ls\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All sources produced identical output!\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Local Adapters': 'ls',\n",
       " 'Local Merged': 'ls',\n",
       " 'HF Adapters': 'ls',\n",
       " 'HF Merged': 'ls'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare outputs from all sources\n",
    "compare_all_sources(\"List all files in current directory\", \"[LINUX]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä COMPARING ALL SOURCES\n",
      "======================================================================\n",
      "Instruction: Delete file named test.log\n",
      "Input: Return the command for all operating systems as JSON\n",
      "======================================================================\n",
      "==================================================\n",
      "üì• Loading: Local LoRA Adapters\n",
      "==================================================\n",
      "\n",
      "‚úÖ Local LoRA Adapters loaded successfully!\n",
      "   VRAM used: 2.44 GB\n",
      "\n",
      "Local Adapters:\n",
      "   ‚ûú {\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}\n",
      "==================================================\n",
      "üì• Loading: Local Merged Model\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 587e2903-b8bc-4fb5-a961-bb6d77a4f04e)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-0.6B/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5824e4ee-5f72-4a7d-8546-d375f34071b2)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-0.6B/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Local Merged Model loaded successfully!\n",
      "   VRAM used: 2.65 GB\n",
      "\n",
      "Local Merged:\n",
      "   ‚ûú {\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}\n",
      "==================================================\n",
      "üì• Loading: HuggingFace LoRA Adapters\n",
      "==================================================\n",
      "\n",
      "‚úÖ HuggingFace LoRA Adapters loaded successfully!\n",
      "   VRAM used: 2.86 GB\n",
      "\n",
      "HF Adapters:\n",
      "   ‚ûú {\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}\n",
      "==================================================\n",
      "üì• Loading: HuggingFace Merged Model\n",
      "==================================================\n",
      "\n",
      "‚úÖ HuggingFace Merged Model loaded successfully!\n",
      "   VRAM used: 3.07 GB\n",
      "\n",
      "HF Merged:\n",
      "   ‚ûú {\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All sources produced identical output!\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Local Adapters': '{\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}',\n",
       " 'Local Merged': '{\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}',\n",
       " 'HF Adapters': '{\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}',\n",
       " 'HF Merged': '{\"description\": \"Delete file named test.log\", \"linux\": \"rm test.log\", \"windows\": \"del test.log\", \"mac\": \"rm test.log\"}'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare JSON output\n",
    "compare_all_sources(\"Delete file named test.log\", \"Return the command for all operating systems as JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model cleared from memory\n"
     ]
    }
   ],
   "source": [
    "# Clear model from memory when done\n",
    "clear_current_model()\n",
    "print(\"‚úÖ Model cleared from memory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
