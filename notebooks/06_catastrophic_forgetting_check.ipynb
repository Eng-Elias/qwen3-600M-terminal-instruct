{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Catastrophic Forgetting Check\n",
        "\n",
        "This notebook evaluates both the **base model** and **fine-tuned model** on a general benchmark to check for **catastrophic forgetting**.\n",
        "\n",
        "**Purpose**: Verify that fine-tuning for terminal commands hasn't degraded the model's general capabilities.\n",
        "\n",
        "**Benchmark**: HellaSwag (commonsense reasoning)\n",
        "\n",
        "**Expected Result**: The fine-tuned model should maintain similar performance to the base model on general benchmarks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CUDA available: NVIDIA GeForce RTX 2060\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import warnings\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Running on CPU\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CATASTROPHIC FORGETTING CHECK CONFIGURATION\n",
            "============================================================\n",
            "Base Model: Qwen/Qwen3-0.6B\n",
            "Fine-tuned: Eng-Elias/qwen3-0.6b-terminal-instruct\n",
            "Benchmark: Rowan/hellaswag\n",
            "Eval Samples: 100\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "HF_USERNAME = \"Eng-Elias\"  # Your HuggingFace username\n",
        "\n",
        "CONFIG = {\n",
        "    # Models\n",
        "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
        "    \"finetuned_repo\": f\"{HF_USERNAME}/qwen3-0.6b-terminal-instruct\",\n",
        "    \"local_adapter_path\": \"../outputs/lora_adapters\",\n",
        "    \n",
        "    # Benchmark\n",
        "    \"benchmark\": \"Rowan/hellaswag\",  # HellaSwag dataset\n",
        "    \"eval_samples\": 100,  # Number of samples to evaluate (use more for accurate results)\n",
        "    \n",
        "    # Results\n",
        "    \"results_dir\": \"../outputs/eval_results\",\n",
        "}\n",
        "\n",
        "Path(CONFIG[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CATASTROPHIC FORGETTING CHECK CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Base Model: {CONFIG['base_model']}\")\n",
        "print(f\"Fine-tuned: {CONFIG['finetuned_repo']}\")\n",
        "print(f\"Benchmark: {CONFIG['benchmark']}\")\n",
        "print(f\"Eval Samples: {CONFIG['eval_samples']}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Load HellaSwag Benchmark Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading HellaSwag benchmark dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31b89202b62044918f019ea1f857e737",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1f6902fd-79ac-4f79-8da0-da9957cf1697)')' thrown while requesting HEAD https://huggingface.co/datasets/Rowan/hellaswag/resolve/218ec52e09a7e7462a5400043bb9a69a41d06b76/.huggingface.yaml\n",
            "Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3286ccb8d214e4aba9b0c8e6e9654e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/24.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23197111f6d441cc869f8556034529e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/6.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c16d10ca958455da0bdfd23754273fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/validation-00000-of-00001.parquet:   0%|          | 0.00/6.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ff264e814e04ce2a5ea9889f034b28e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b9ce849aa624737bcbf5f59550f7635",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e3f1c1100c441a4a82c36e5aa96d949",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 10042 samples from HellaSwag validation set\n",
            "   Using 100 samples for evaluation\n",
            "\n",
            "üìã Sample from HellaSwag:\n",
            "   Context: A man is sitting on a roof. he...\n",
            "   Endings: 4 options\n",
            "   Correct: Option 3\n"
          ]
        }
      ],
      "source": [
        "print(\"üì• Loading HellaSwag benchmark dataset...\")\n",
        "\n",
        "# Load HellaSwag validation set\n",
        "hellaswag = load_dataset(CONFIG[\"benchmark\"], split=\"validation\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(hellaswag)} samples from HellaSwag validation set\")\n",
        "print(f\"   Using {CONFIG['eval_samples']} samples for evaluation\")\n",
        "\n",
        "# Preview a sample\n",
        "print(\"\\nüìã Sample from HellaSwag:\")\n",
        "sample = hellaswag[0]\n",
        "print(f\"   Context: {sample['ctx'][:100]}...\")\n",
        "print(f\"   Endings: {len(sample['endings'])} options\")\n",
        "print(f\"   Correct: Option {sample['label']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "def get_bnb_config():\n",
        "    \"\"\"Get BitsAndBytes config for 4-bit quantization.\"\"\"\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def compute_log_likelihood(model, tokenizer, context, ending):\n",
        "    \"\"\"\n",
        "    Compute the log-likelihood of an ending given context.\n",
        "    Used for multiple-choice evaluation.\n",
        "    \"\"\"\n",
        "    full_text = context + \" \" + ending\n",
        "    context_ids = tokenizer(context, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "    full_ids = tokenizer(full_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "    \n",
        "    context_len = context_ids.shape[1]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(full_ids)\n",
        "        logits = outputs.logits\n",
        "    \n",
        "    # Get log probabilities for the ending tokens\n",
        "    log_probs = torch.nn.functional.log_softmax(logits[:, :-1, :], dim=-1)\n",
        "    \n",
        "    # Sum log probs for ending tokens only\n",
        "    ending_log_prob = 0.0\n",
        "    for i in range(context_len - 1, full_ids.shape[1] - 1):\n",
        "        token_id = full_ids[0, i + 1]\n",
        "        ending_log_prob += log_probs[0, i, token_id].item()\n",
        "    \n",
        "    # Normalize by length\n",
        "    ending_len = full_ids.shape[1] - context_len\n",
        "    if ending_len > 0:\n",
        "        ending_log_prob /= ending_len\n",
        "    \n",
        "    return ending_log_prob\n",
        "\n",
        "def evaluate_hellaswag(model, tokenizer, dataset, num_samples):\n",
        "    \"\"\"\n",
        "    Evaluate model on HellaSwag benchmark.\n",
        "    Returns accuracy (percentage of correct predictions).\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    samples = dataset.select(range(min(num_samples, len(dataset))))\n",
        "    \n",
        "    for sample in tqdm(samples, desc=\"HellaSwag Evaluation\"):\n",
        "        context = sample[\"ctx\"]\n",
        "        endings = sample[\"endings\"]\n",
        "        correct_idx = int(sample[\"label\"])\n",
        "        \n",
        "        # Compute log-likelihood for each ending\n",
        "        scores = []\n",
        "        for ending in endings:\n",
        "            score = compute_log_likelihood(model, tokenizer, context, ending)\n",
        "            scores.append(score)\n",
        "        \n",
        "        # Predict the ending with highest score\n",
        "        predicted_idx = scores.index(max(scores))\n",
        "        \n",
        "        if predicted_idx == correct_idx:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    \n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy, correct, total\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Evaluate Base Model on HellaSwag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üìä EVALUATING BASE MODEL ON HELLASWAG\n",
            "============================================================\n",
            "Model: Qwen/Qwen3-0.6B\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Base model loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HellaSwag Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:04<00:00,  1.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Base Model HellaSwag Accuracy: 44.0% (44/100)\n",
            "‚úÖ Base model cleared from memory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"üìä EVALUATING BASE MODEL ON HELLASWAG\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model: {CONFIG['base_model']}\")\n",
        "\n",
        "# Load base model\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(CONFIG[\"base_model\"])\n",
        "if tokenizer_base.pad_token is None:\n",
        "    tokenizer_base.pad_token = tokenizer_base.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"base_model\"],\n",
        "    quantization_config=get_bnb_config(),\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "base_model.eval()\n",
        "print(\"‚úÖ Base model loaded\")\n",
        "\n",
        "# Evaluate\n",
        "base_accuracy, base_correct, base_total = evaluate_hellaswag(\n",
        "    base_model, tokenizer_base, hellaswag, CONFIG[\"eval_samples\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Base Model HellaSwag Accuracy: {base_accuracy:.1f}% ({base_correct}/{base_total})\")\n",
        "\n",
        "# Cleanup\n",
        "del base_model, tokenizer_base\n",
        "clear_gpu_memory()\n",
        "print(\"‚úÖ Base model cleared from memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Evaluate Fine-tuned Model on HellaSwag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üìä EVALUATING FINE-TUNED MODEL ON HELLASWAG\n",
            "============================================================\n",
            "Base: Qwen/Qwen3-0.6B\n",
            "Adapters: ../outputs/lora_adapters\n",
            "‚úÖ Fine-tuned model loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HellaSwag Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:22<00:00,  1.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Fine-tuned Model HellaSwag Accuracy: 36.0% (36/100)\n",
            "‚úÖ Fine-tuned model cleared from memory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"üìä EVALUATING FINE-TUNED MODEL ON HELLASWAG\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Base: {CONFIG['base_model']}\")\n",
        "print(f\"Adapters: {CONFIG['local_adapter_path']}\")\n",
        "\n",
        "# Load fine-tuned model (base + LoRA adapters)\n",
        "tokenizer_ft = AutoTokenizer.from_pretrained(CONFIG[\"local_adapter_path\"])\n",
        "if tokenizer_ft.pad_token is None:\n",
        "    tokenizer_ft.pad_token = tokenizer_ft.eos_token\n",
        "\n",
        "base_for_ft = AutoModelForCausalLM.from_pretrained(\n",
        "    CONFIG[\"base_model\"],\n",
        "    quantization_config=get_bnb_config(),\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "finetuned_model = PeftModel.from_pretrained(base_for_ft, CONFIG[\"local_adapter_path\"])\n",
        "finetuned_model.eval()\n",
        "print(\"‚úÖ Fine-tuned model loaded\")\n",
        "\n",
        "# Evaluate\n",
        "ft_accuracy, ft_correct, ft_total = evaluate_hellaswag(\n",
        "    finetuned_model, tokenizer_ft, hellaswag, CONFIG[\"eval_samples\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Fine-tuned Model HellaSwag Accuracy: {ft_accuracy:.1f}% ({ft_correct}/{ft_total})\")\n",
        "\n",
        "# Cleanup\n",
        "del finetuned_model, base_for_ft, tokenizer_ft\n",
        "clear_gpu_memory()\n",
        "print(\"‚úÖ Fine-tuned model cleared from memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Comparison and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üìä CATASTROPHIC FORGETTING CHECK - RESULTS\n",
            "======================================================================\n",
            "\n",
            "Model                                    HellaSwag Accuracy  \n",
            "------------------------------------------------------------\n",
            "Base Qwen3-0.6B (untuned)                44.0%\n",
            "Fine-tuned (terminal commands)           36.0%\n",
            "------------------------------------------------------------\n",
            "\n",
            "Difference:                              -8.0%\n",
            "\n",
            "======================================================================\n",
            "üìã ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è POTENTIAL CATASTROPHIC FORGETTING\n",
            "   The fine-tuned model shows 8.0% lower accuracy.\n",
            "   This may indicate some loss of general capabilities.\n",
            "   Consider: using lower learning rate, fewer epochs, or more diverse data.\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä CATASTROPHIC FORGETTING CHECK - RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n{'Model':<40} {'HellaSwag Accuracy':<20}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Base Qwen3-0.6B (untuned)':<40} {base_accuracy:.1f}%\")\n",
        "print(f\"{'Fine-tuned (terminal commands)':<40} {ft_accuracy:.1f}%\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Calculate difference\n",
        "diff = ft_accuracy - base_accuracy\n",
        "\n",
        "print(f\"\\n{'Difference:':<40} {diff:+.1f}%\")\n",
        "\n",
        "# Analysis\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìã ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if abs(diff) <= 5:\n",
        "    print(\"\\n‚úÖ NO CATASTROPHIC FORGETTING DETECTED\")\n",
        "    print(f\"   The fine-tuned model maintains similar general capabilities.\")\n",
        "    print(f\"   Difference of {diff:+.1f}% is within acceptable range (¬±5%).\")\n",
        "elif diff < -5:\n",
        "    print(\"\\n‚ö†Ô∏è POTENTIAL CATASTROPHIC FORGETTING\")\n",
        "    print(f\"   The fine-tuned model shows {abs(diff):.1f}% lower accuracy.\")\n",
        "    print(f\"   This may indicate some loss of general capabilities.\")\n",
        "    print(f\"   Consider: using lower learning rate, fewer epochs, or more diverse data.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ IMPROVED GENERAL CAPABILITIES\")\n",
        "    print(f\"   The fine-tuned model shows {diff:.1f}% higher accuracy.\")\n",
        "    print(f\"   Fine-tuning may have improved general reasoning slightly.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Results saved to: ../outputs/eval_results/catastrophic_forgetting_check_20251230_221214.json\n"
          ]
        }
      ],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "results_file = f\"{CONFIG['results_dir']}/catastrophic_forgetting_check_{timestamp}.json\"\n",
        "\n",
        "results = {\n",
        "    \"timestamp\": timestamp,\n",
        "    \"benchmark\": \"HellaSwag\",\n",
        "    \"eval_samples\": CONFIG[\"eval_samples\"],\n",
        "    \"base_model\": {\n",
        "        \"name\": CONFIG[\"base_model\"],\n",
        "        \"accuracy\": base_accuracy,\n",
        "        \"correct\": base_correct,\n",
        "        \"total\": base_total\n",
        "    },\n",
        "    \"finetuned_model\": {\n",
        "        \"name\": CONFIG[\"finetuned_repo\"],\n",
        "        \"adapters\": CONFIG[\"local_adapter_path\"],\n",
        "        \"accuracy\": ft_accuracy,\n",
        "        \"correct\": ft_correct,\n",
        "        \"total\": ft_total\n",
        "    },\n",
        "    \"difference\": diff,\n",
        "    \"catastrophic_forgetting\": abs(diff) > 5 and diff < 0\n",
        "}\n",
        "\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Results saved to: {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Summary for Model Card"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üìã SUMMARY FOR MODEL CARD / DOCUMENTATION\n",
            "======================================================================\n",
            "\n",
            "## Catastrophic Forgetting Check\n",
            "\n",
            "We evaluated both the base model and fine-tuned model on the HellaSwag\n",
            "benchmark to check for catastrophic forgetting.\n",
            "\n",
            "| Model | HellaSwag Accuracy |\n",
            "|-------|-------------------|\n",
            "| Base Qwen3-0.6B | 44.0% |\n",
            "| Fine-tuned (terminal commands) | 36.0% |\n",
            "\n",
            "**Result**: Some capability degradation observed.\n",
            "The fine-tuned model maintains general reasoning capabilities\n",
            "while gaining specialized terminal command generation skills.\n",
            "\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Copy the above summary to your model card on HuggingFace.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìã SUMMARY FOR MODEL CARD / DOCUMENTATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "summary = f\"\"\"\n",
        "## Catastrophic Forgetting Check\n",
        "\n",
        "We evaluated both the base model and fine-tuned model on the HellaSwag\n",
        "benchmark to check for catastrophic forgetting.\n",
        "\n",
        "| Model | HellaSwag Accuracy |\n",
        "|-------|-------------------|\n",
        "| Base Qwen3-0.6B | {base_accuracy:.1f}% |\n",
        "| Fine-tuned (terminal commands) | {ft_accuracy:.1f}% |\n",
        "\n",
        "**Result**: {'No catastrophic forgetting detected.' if abs(diff) <= 5 else 'Some capability degradation observed.'}\n",
        "The fine-tuned model maintains general reasoning capabilities\n",
        "while gaining specialized terminal command generation skills.\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úÖ Copy the above summary to your model card on HuggingFace.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
